{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Segment_game.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LN_nPKlkQuWJ",
        "colab_type": "code",
        "outputId": "683b16f8-37c1-4e43-a993-02a7c18f8639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import imageio\n",
        "\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S3R2JzOnRTQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ]
    },
    {
      "metadata": {
        "id": "L1cyx0u3LfAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###\n",
        "SEGMENT_LENGTH = 20\n",
        "EXPLORATION_COST = 0.1\n",
        "PRED_REWARD = SEGMENT_LENGTH*0.1/2\n",
        "###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvIF8O5NQ3DC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_a_segment(length=50, noise=0.0, free_location=False):\n",
        "  if free_location:\n",
        "    l1 = np.random.randint(1, length-1)\n",
        "    l2 = np.random.randint(1, length-1)\n",
        "    left = min([l1, l2])\n",
        "    right = max([l1, l2])+1\n",
        "    segment = [0]*left + [1]*(right-left) + [0]*(length-right)\n",
        "  else:\n",
        "    right = np.random.randint(1, length)\n",
        "    segment = [0]*right + [1]*(length-right)\n",
        "  segment = np.array(segment) + noise * np.random.random(length)\n",
        "  return segment, float(right)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WMaMyv93RRuw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ImgEnv(object):\n",
        "    def __init__(self, max_steps=1000, window=1, segment_length=50, noise=0.0, free_location=False,\n",
        "                 expl_cost=0.05, pred_reward=1):\n",
        "      self.pred_reward = pred_reward\n",
        "      self.free_location = free_location\n",
        "      self.noise = noise\n",
        "      self.segment_length = segment_length\n",
        "      self.action_space = (Discrete(self.segment_length), Discrete(self.segment_length))\n",
        "      self.observation_space = Discrete(self.segment_length)\n",
        "      self.window = window\n",
        "      self.max_steps = max_steps\n",
        "      self.to_draw = np.zeros((self.max_steps+1, 1, self.segment_length, 3)).astype(int)\n",
        "      self.expl_cost = expl_cost\n",
        "      self.window = window\n",
        "\n",
        "    def seed(self, seed):\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def reset(self, NEXT=True):\n",
        "      self.pos = None\n",
        "      self.num_steps=0\n",
        "      if NEXT:\n",
        "        self.curr_img, self.target = generate_a_segment(length=self.segment_length, noise=self.noise, free_location=self.free_location)\n",
        "\n",
        "      self.state = np.zeros((2, self.segment_length)).astype(int)\n",
        "      return self.state\n",
        "\n",
        "    \n",
        "    def get_frame(self,t,pred=None):\n",
        "        true_image = np.zeros((1, self.segment_length, 3)).astype(int)\n",
        "        true_image[:, self.curr_img==1, 2] = 255\n",
        "        true_image[:, self.curr_img==0, 0] = 255\n",
        "\n",
        "        segment_plot = np.zeros((1, self.segment_length, 3)).astype(int)+128\n",
        "        segment_plot[:, self.state[0]==1, :] = true_image[:, self.state[0]==1, :]\n",
        "        if self.pos is not None:\n",
        "            segment_plot[:, self.pos, :] = np.clip(segment_plot[:, self.pos, :] +170, 0, 255)\n",
        "        if pred is not None:\n",
        "            segment_plot[:, pred, 1] = np.clip(segment_plot[:, pred, 1] + 170, 0, 255)\n",
        "        \n",
        "        self.to_draw[t,:,:,:] = segment_plot\n",
        "        \n",
        "    def draw(self,e):\n",
        "        true_image = np.zeros((1, self.segment_length, 3)).astype(int)\n",
        "        true_image[:, self.curr_img==1, 2] = 255\n",
        "        true_image[:, self.curr_img==0, 0] = 255\n",
        "\n",
        "        array_list = [\n",
        "            np.vstack([s_plot, true_image]) for s_plot in self.to_draw[:self.num_steps]\n",
        "        ]\n",
        "        image_list = []\n",
        "        for a in array_list:\n",
        "            fig = plt.figure()\n",
        "            plt.yticks([])\n",
        "            plt.imshow(a)\n",
        "            plt.hlines(0.5, -0.5, self.segment_length-0.5)\n",
        "            fig.canvas.draw()\n",
        "            image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
        "            image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "            plt.close(fig)\n",
        "            image_list.append(image)\n",
        "        imageio.mimsave('./{}.gif'.format(e), image_list, fps=1)\n",
        "       \n",
        "    def step(self, action):\n",
        "      \"\"\"\n",
        "      \n",
        "      \"\"\"\n",
        "      \n",
        "      \"\"\"if self.num_steps >= self.max_steps:\n",
        "        done = True\n",
        "        reward = self.target == np.argmax(action[1, :])\n",
        "        self.get_frame(int(self.num_steps))\n",
        "        self.num_steps += 1\n",
        "        return self.state, reward, done\n",
        "      \n",
        "      action = np.unravel_index(action.argmax(), action.shape)\n",
        "      \"\"\"\n",
        "      if action[0]==0:  \n",
        "          self.pos = action[1]\n",
        "          done = False\n",
        "          reward = -self.expl_cost\n",
        "          self.state[1, max(self.pos - self.window, 0):min(self.pos + self.window + 1, self.segment_length)] = self.curr_img[max(self.pos - self.window, 0):min(self.pos + self.window + 1, self.segment_length)]\n",
        "          self.state[0, max(self.pos - self.window, 0):min(self.pos + self.window + 1, self.segment_length)] = np.ones(min(self.pos + self.window + 1, self.segment_length) - max(self.pos - self.window, 0)).astype(int)\n",
        "          pred = None\n",
        "      else:\n",
        "          done = True\n",
        "          reward = (self.target == action[1]).item()*self.pred_reward\n",
        "          pred = action[1]\n",
        "          \n",
        "      self.get_frame(int(self.num_steps), pred=pred)\n",
        "      self.num_steps += 1\n",
        "      return self.state, reward, done\n",
        "\n",
        "    def get_current_obs(self):\n",
        "        return self.state\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BcMVfRG3RXU8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ]
    },
    {
      "metadata": {
        "id": "BntPsQSrRa7l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, epsilon, segment_length, random_can_stop=False):\n",
        "        self.epsilon = epsilon\n",
        "        self.segment_length = segment_length\n",
        "        self.random_can_stop=random_can_stop\n",
        "    \n",
        "    def set_epsilon(self,e):\n",
        "        self.epsilon = e\n",
        "\n",
        "    def act(self,s,train=True, must_stop=False):\n",
        "        \"\"\" This function should return the next action to do:\n",
        "        an array of length self.segment_length with a random exploration of epsilon\"\"\"\n",
        "        if train and np.random.rand() <= self.epsilon:\n",
        "            if must_stop:\n",
        "                return torch.tensor([1, random.randrange(self.segment_length)], device=device, dtype=torch.long)\n",
        "            if self.random_can_stop:\n",
        "                return torch.tensor([random.randrange(2), random.randrange(self.segment_length)], device=device, dtype=torch.long)\n",
        "            return torch.tensor([0, random.randrange(self.segment_length)], device=device, dtype=torch.long)\n",
        "        a = self.learned_act(s)        \n",
        "        if must_stop:\n",
        "            return torch.tensor([1, torch.argmax(a[:, 1, :])], device=device, dtype=torch.long)\n",
        "        max_index = torch.argmax(a)\n",
        "        return torch.tensor([max_index / self.segment_length, max_index % self.segment_length], device=device, dtype=torch.long)\n",
        "\n",
        "    def learned_act(self,s):\n",
        "        \"\"\" Act via the policy of the agent, from a given state s\n",
        "        it proposes an action a\"\"\"\n",
        "        pass\n",
        "\n",
        "    def reinforce(self, s, n_s, a, r, game_over_):\n",
        "        \"\"\" This function is the core of the learning algorithm. \n",
        "        It takes as an input the current state s_, the next state n_s_\n",
        "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
        "        \n",
        "        Its goal is to learn a policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\" This function returns basic stats if applicable: the\n",
        "        loss and/or the model\"\"\"\n",
        "        pass\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\" This function allows to restore a model\"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uL6FeRMORcVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Memory Replay\n",
        "from collections import namedtuple\n",
        "import random\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'game_over'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)   \n",
        "     \n",
        "    def remember(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if batch_size <= self.__len__():\n",
        "            return random.sample(self.memory, batch_size)\n",
        "        return random.choices(self.memory, k=batch_size) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xegXTqHsRj4H",
        "colab_type": "code",
        "outputId": "c44f3505-b429-4bcd-82e0-d25adb1db007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import imageio\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class nav_and_pred_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(nav_and_pred_model, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(2, 64, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv1d(64, 3, kernel_size=3, padding=1)\n",
        "    self.linear1 = nn.Linear(60, 60)\n",
        "    self.linear2 = nn.Linear(60, 40)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    #print(x.shape)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    #print(x.shape)\n",
        "    x = self.linear1(x)\n",
        "    #print(x.shape)\n",
        "    x = self.linear2(x)\n",
        "    #print(x.shape)\n",
        "    x = x.view(x.size(0), 2, -1)\n",
        "    #print(x.shape)\n",
        "    return x\n",
        "\n",
        "class navigation_model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(navigation_model, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(2, 64, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "    self.linear1 = nn.Linear(64*SEGMENT_LENGTH, SEGMENT_LENGTH)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    #print(x.shape)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    #print(x.shape)\n",
        "    x = self.linear1(x)\n",
        "    #print(x.shape)\n",
        "    x = x.view(x.size(0), 1, -1)\n",
        "    #print(x.shape)\n",
        "    return x  \n",
        "  \n",
        "class assembled_model(nn.Module):\n",
        "  def __init__(self, oracle=True):\n",
        "    super(assembled_model, self).__init__()\n",
        "    \n",
        "  def forward(self, s):\n",
        "    #print(s.shape)\n",
        "    x = navigation_model(s)\n",
        "    #print(x.shape)\n",
        "    return x\n",
        "\n",
        "model_full = nav_and_pred_model()\n",
        "navigation_model = navigation_model()\n",
        "assembled_model = assembled_model()\n",
        "\n",
        "if use_cuda:\n",
        "  print('Using GPU')\n",
        "  model_full.cuda()\n",
        "  navigation_model.cuda()\n",
        "  assembled_model.cuda()\n",
        "else:\n",
        "  print('Using CPU')\n",
        "  \n",
        "optimizer_full = optim.RMSprop(model_full.parameters(), lr=1e-3)\n",
        "optimizer_navigation = optim.RMSprop(navigation_model.parameters(), lr=1e-5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yy4hI7RhU6BR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def oracle_array(state):\n",
        "    segment_length = state.shape[-1]\n",
        "    first_1_index_list = np.where(state[1, :]==1)[0]\n",
        "    last_0_index_list = np.where((state[1, :]==0) & (state[0, :]==1))[0]\n",
        "    if len(first_1_index_list) == 0:\n",
        "        first_1_index = segment_length-1\n",
        "    else:\n",
        "        first_1_index = first_1_index_list[0]\n",
        "    if len(last_0_index_list) == 0:\n",
        "        last_0_index = 0\n",
        "    else:\n",
        "        last_0_index = last_0_index_list[-1]\n",
        "    return torch.tensor([0.]*(last_0_index + 1) + [1.]*(first_1_index - last_0_index) + [0.]*(segment_length-first_1_index - 1), device=device)/(first_1_index - last_0_index)\n",
        "\n",
        "def oracle(s_batch):\n",
        "    state_cpu = s_batch.cpu().numpy()\n",
        "    return torch.cat([oracle_array(s).unsqueeze(0) for s in state_cpu])*PRED_REWARD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KIRZjn47ytGn",
        "colab_type": "code",
        "outputId": "380e9c63-313d-47e7-fa39-b058d7af55f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "ex = np.array([\n",
        "    \n",
        "    [[0,0,0,1,0,1],\n",
        "     [0,0,0,1,0,1]],\n",
        "    [[0,0,1,0,0,1],\n",
        "     [0,0,1,0,0,1]]\n",
        "    \n",
        "])\n",
        "ex2 = np.array([\n",
        "    \n",
        "    [1,1,0,1,1,0],\n",
        "    [0,0,0,1,1,0]\n",
        "    \n",
        "])\n",
        "\n",
        "print(oracle(torch.tensor(ex, device=device)))\n",
        "oracle_array(ex2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0000, 1.6667, 1.6667, 1.6667, 0.0000, 0.0000],\n",
            "        [0.0000, 2.5000, 2.5000, 0.0000, 0.0000, 0.0000]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0000, 0.5000, 0.5000, 0.0000, 0.0000], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "miyBffpsRxqg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN_separated_net(Agent):\n",
        "    def __init__(self, epsilon=0.3, segment_length=50, memory_size=300, batch_size = 16):\n",
        "        super(DQN_separated_net, self).__init__(epsilon=epsilon, segment_length=segment_length, random_can_stop=False)\n",
        "\n",
        "        # Memory\n",
        "        self.memory = ReplayMemory(memory_size)\n",
        "        \n",
        "        # Batch size when learning\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def learned_act(self, s, pred_oracle=True, requ_grad=False):\n",
        "        if requ_grad:\n",
        "            if pred_oracle:\n",
        "                return torch.cat([navigation_model(s), oracle(s).unsqueeze(1)], 1)\n",
        "        with torch.no_grad():\n",
        "            if pred_oracle:\n",
        "                return torch.cat([navigation_model(s), oracle(s).unsqueeze(1)], 1)\n",
        "        #to do without oracle\n",
        "\n",
        "    def reinforce(self, s_, a_, n_s_, r_, game_over_):\n",
        "        # Two steps: first memorize the states, second learn from the pool\n",
        "\n",
        "        self.memory.remember(s_, a_, n_s_, r_, game_over_)\n",
        "        \n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # Compute a mask of non-final states and concatenate the batch elements\n",
        "        # (a final state would've been the one after which simulation ended)\n",
        "        \n",
        "        \n",
        "        non_final_mask = torch.tensor(torch.cat(batch.game_over), device=device)==False\n",
        "        non_final_next_states = torch.cat(batch.next_state)[non_final_mask]\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action).view(-1, 2)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        #non_final_next_states = torch.cat(batch.next_state)[non_final_index]\n",
        "        \n",
        "        # print(state_batch.shape)\n",
        "        state_values = self.learned_act(state_batch, requ_grad=True)\n",
        "        state_action_values = torch.cat([s[a[0].item(), a[1].item()].unsqueeze(0) for s, a in zip(state_values, batch.action)])\n",
        "        \n",
        "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
        "        \n",
        "        if len(non_final_next_states)>0:\n",
        "            next_state_values[non_final_mask] = (self.learned_act(non_final_next_states, requ_grad=False).max(2)[0]).max(1)[0]\n",
        "        expected_state_action_values = next_state_values + reward_batch \n",
        "\n",
        "        # print(state_action_values.shape)\n",
        "        # print(expected_state_action_values.shape)\n",
        "        \n",
        "        loss = F.smooth_l1_loss(state_action_values[non_final_mask], expected_state_action_values[non_final_mask])#.unsqueeze(1))\n",
        "\n",
        "        # Optimize the model\n",
        "        optimizer_navigation.zero_grad()\n",
        "        loss.backward()\n",
        "        for param in navigation_model.parameters():\n",
        "            # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        optimizer_navigation.step()\n",
        "        return float(loss)\n",
        "\n",
        "    \"\"\"\n",
        "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
        "        self.model.save_weights(name_weights, overwrite=True)\n",
        "        with open(name_model, \"w\") as outfile:\n",
        "            json.dump(self.model.to_json(), outfile)\n",
        "            \n",
        "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
        "        with open(name_model, \"r\") as jfile:\n",
        "            model = model_from_json(json.load(jfile))\n",
        "        model.load_weights(name_weights)\n",
        "        model.compile(\"adam\", \"mse\")\n",
        "        self.model = model\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbiJwfVD9Ap9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "state_test = torch.zeros((1, 2, SEGMENT_LENGTH), device=device)\n",
        "#state_test[0, :, int(SEGMENT_LENGTH/2):SEGMENT_LENGTH] = torch.ones((2, SEGMENT_LENGTH-int(SEGMENT_LENGTH/2)), device=device)\n",
        "\n",
        "\n",
        "#state_test[0, 0, 0:5] = torch.ones(5, device=device)\n",
        "\n",
        "print(state_test)\n",
        "agent.learned_act(state_test)#[0][0][10].item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SKlizEENVW6_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train & Test"
      ]
    },
    {
      "metadata": {
        "id": "f4FH4Hq5VdNP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(agent,env,epoch,prefix='', draw=False, next_after_e=True):\n",
        "    # Number of won games\n",
        "    loss_list = []\n",
        "    reward_list = []\n",
        "    score = 0\n",
        "    loss = 0\n",
        "    for e in range(epoch):\n",
        "        # At each epoch, we restart to a fresh game and get the initial state\n",
        "        state = env.reset(NEXT=next_after_e)\n",
        "        state = torch.tensor(state, device=device, dtype=torch.float).unsqueeze(0)\n",
        "        # This assumes that the games will terminate\n",
        "        game_over = False\n",
        "\n",
        "        tot_reward = 0\n",
        "        tot_loss = 0\n",
        "        while not game_over:\n",
        "            # The agent performs an action\n",
        "            action = agent.act(state, must_stop=env.num_steps >= env.max_steps)\n",
        "            # Apply an action to the environment, get the next state, the reward\n",
        "            # and if the games end\n",
        "            prev_state = state\n",
        "            state, reward, game_over = env.step(action)\n",
        "            tot_reward += reward\n",
        "            \n",
        "            #action.view(1, 2, -1) #########\n",
        "            state = torch.tensor(state, device=device, dtype=torch.float).unsqueeze(0)\n",
        "            reward = torch.tensor(reward, device=device, dtype=torch.float).unsqueeze(0)\n",
        "            game_over_tensor = torch.tensor(game_over, device=device).unsqueeze(0)\n",
        "            # Update the counters\n",
        "\n",
        "            # Apply the reinforcement strategy\n",
        "            loss = agent.reinforce(prev_state, action, state, reward, game_over_tensor)\n",
        "            tot_loss += loss\n",
        "            #print((e, epoch, loss, tot_reward))\n",
        "            # Save action\n",
        "            if draw:\n",
        "                env.draw(prefix+str(e))\n",
        "\n",
        "        # Update stats\n",
        "        score += tot_reward\n",
        "        loss_list.append(tot_loss/env.num_steps)\n",
        "        reward_list.append(tot_reward)\n",
        "        \n",
        "        if e%50 ==0:\n",
        "            print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | reward {}\".format(e, epoch, loss, tot_reward)) \n",
        "            print(agent.learned_act(state_test)[0][0][10].item())\n",
        "            #agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
        "        \n",
        "    with open(\"/content/drive/My Drive/CEPchallenge/rewards_{}.pickle\".format(prefix), 'wb') as handle:\n",
        "        pickle.dump(reward_list, handle)\n",
        "\n",
        "    with open(\"/content/drive/My Drive/CEPchallenge/losses_{}.pickle\".format(prefix), 'wb') as handle:\n",
        "        pickle.dump(loss_list, handle)\n",
        "        \n",
        "        \n",
        "def test(agent, env,epochs, prefix='', next_after_e=True, draw=False, path_save='/drive/My Drive/CEPchallenge/gif_segment/'):\n",
        "    # Number of won games\n",
        "    score = 0\n",
        "    loss_list = []\n",
        "    reward_list = []\n",
        "    for e in range(epochs):\n",
        "        state = env.reset(NEXT=next_after_e)\n",
        "        state = torch.tensor(state, device=device, dtype=torch.float).unsqueeze(0)\n",
        "        # This assumes that the games will terminate\n",
        "        game_over = False\n",
        "        tot_reward = 0\n",
        "        tot_loss = 0\n",
        "        while not game_over:\n",
        "            must_stop = env.num_steps >= env.max_steps\n",
        "            action = agent.act(state, must_stop=must_stop, train=False)\n",
        "            state, reward, game_over = env.step(action)\n",
        "            \n",
        "            tot_reward += reward\n",
        "            \n",
        "            #action.view(1, 2, -1) #########\n",
        "            state = torch.tensor(state, device=device, dtype=torch.float).unsqueeze(0)\n",
        "            reward = torch.tensor(reward, device=device, dtype=torch.float).unsqueeze(0)\n",
        "            game_over_tensor = torch.tensor(game_over, device=device).unsqueeze(0)\n",
        "                        \n",
        "        # Save as a mp4\n",
        "        if draw:\n",
        "            env.draw(path_save + prefix + str(e))\n",
        "    \n",
        "        # Update stats\n",
        "        score = score + tot_reward\n",
        "        \n",
        "        loss_list.append(tot_loss/env.num_steps)\n",
        "        reward_list.append(tot_reward)\n",
        "        print(\"epoch {} : total reward = {}\".format(e, tot_reward))\n",
        "      \n",
        "        with open(\"/content/drive/My Drive/CEPchallenge/rewards_test_{}.pickle\".format(prefix), 'wb') as handle:\n",
        "            pickle.dump(reward_list, handle)\n",
        "  \n",
        "        with open(\"/content/drive/My Drive/CEPchallenge/losses_test_{}.pickle\".format(prefix), 'wb') as handle:\n",
        "            pickle.dump(loss_list, handle)\n",
        "    print('Final score: '+str(score/epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DbzaAjYDWNtQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SEGMENT_LENGTH\n",
        "\n",
        "batch_size = 16\n",
        "memory_size = 50\n",
        "\n",
        "#Initialize the environment!\n",
        "env = ImgEnv(max_steps=SEGMENT_LENGTH+5, window=0, segment_length=SEGMENT_LENGTH, expl_cost=0.05,\n",
        "            pred_reward=1.)\n",
        "\n",
        "env.reset()\n",
        "\n",
        "# Initialize the agent!\n",
        "#agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.1, memory_size=50)\n",
        "\n",
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.3, memory_size=memory_size, batch_size=batch_size)\n",
        "train(agent,env,2000,prefix='DQN0_to_2000', next_after_e=True, draw=False)\n",
        "test(agent,env,20,prefix='DQN0_to_2000', next_after_e=True, draw=True)\n",
        "\n",
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.2, memory_size=memory_size, batch_size=batch_size)\n",
        "train(agent,env,3000,prefix='DQN2000_to_5000', next_after_e=True, draw=False)\n",
        "test(agent,env,20,prefix='DQN2000_to_5000', next_after_e=True, draw=True)\n",
        "\n",
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.1, memory_size=memory_size, batch_size=batch_size)\n",
        "train(agent,env,10000,prefix='DQN5000_to_15000', next_after_e=True, draw=False)\n",
        "test(agent,env,20,prefix='DQN5000_to_15000', next_after_e=True, draw=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LJBEddzyJvM_",
        "colab_type": "code",
        "outputId": "c9b80146-8d80-4945-d22c-fae6e011e7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.3, memory_size=memory_size, batch_size=batch_size)\n",
        "train(agent,env,2000,prefix='DQN', next_after_e=True, draw=False)\n",
        "test(agent,env,5,prefix='DQN', next_after_e=True, draw=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 : total reward = -10.499999999999979\n",
            "epoch 1 : total reward = -10.499999999999979\n",
            "epoch 2 : total reward = -10.499999999999979\n",
            "epoch 3 : total reward = -10.499999999999979\n",
            "epoch 4 : total reward = -10.499999999999979\n",
            "Final score: -10.499999999999979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mKblNAzdoiiW",
        "colab_type": "code",
        "outputId": "f4306e97-5953-4438-c4ae-631fb11a509c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11682
        }
      },
      "cell_type": "code",
      "source": [
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.3, memory_size=memory_size, batch_size=batch_size)\n",
        "\n",
        "test(agent,env,20,prefix='DQN0', next_after_e=True, draw=True)\n",
        "\n",
        "train(agent,env,2000,prefix='DQN0_to_2000', next_after_e=True, draw=False)\n",
        "test(agent,env,20,prefix='DQN0_to_2000', next_after_e=True, draw=True)\n",
        "\n",
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.2, memory_size=memory_size, batch_size=batch_size)\n",
        "train(agent,env,3000,prefix='DQN2000_to_5000', next_after_e=True, draw=False)\n",
        "test(agent,env,20,prefix='DQN2000_to_5000', next_after_e=True, draw=True)\n",
        "\n",
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.1, memory_size=memory_size, batch_size=batch_size)\n",
        "train(agent,env,10000,prefix='DQN5000_to_15000', next_after_e=True, draw=False)\n",
        "test(agent,env,20,prefix='DQN5000_to_15000', next_after_e=True, draw=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 : total reward = -0.05\n",
            "epoch 1 : total reward = -0.05\n",
            "epoch 2 : total reward = -0.05\n",
            "epoch 3 : total reward = -0.05\n",
            "epoch 4 : total reward = -0.05\n",
            "epoch 5 : total reward = -0.05\n",
            "epoch 6 : total reward = -0.05\n",
            "epoch 7 : total reward = -0.05\n",
            "epoch 8 : total reward = -0.05\n",
            "epoch 9 : total reward = -0.05\n",
            "epoch 10 : total reward = -0.05\n",
            "epoch 11 : total reward = -0.05\n",
            "epoch 12 : total reward = -0.05\n",
            "epoch 13 : total reward = -0.05\n",
            "epoch 14 : total reward = -0.05\n",
            "epoch 15 : total reward = -0.05\n",
            "epoch 16 : total reward = -0.05\n",
            "epoch 17 : total reward = -0.05\n",
            "epoch 18 : total reward = -0.05\n",
            "epoch 19 : total reward = -0.05\n",
            "Final score: -0.05000000000000001\n",
            "Epoch 000/2000 | Loss 0.0003 | reward -0.05\n",
            "-0.0335487425327301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 050/2000 | Loss 0.0055 | reward 0.0\n",
            "-0.01738676428794861\n",
            "Epoch 100/2000 | Loss 0.0003 | reward -0.05\n",
            "-0.00027565378695726395\n",
            "Epoch 150/2000 | Loss 0.0003 | reward -0.15000000000000002\n",
            "0.018405666574835777\n",
            "Epoch 200/2000 | Loss 0.0027 | reward -0.15000000000000002\n",
            "0.05279618501663208\n",
            "Epoch 250/2000 | Loss 0.0009 | reward -0.05\n",
            "0.09053246676921844\n",
            "Epoch 300/2000 | Loss 0.0038 | reward -0.05\n",
            "0.07119772583246231\n",
            "Epoch 350/2000 | Loss 0.0009 | reward -0.05\n",
            "0.07513059675693512\n",
            "Epoch 400/2000 | Loss 0.0025 | reward 0.9\n",
            "0.08406300842761993\n",
            "Epoch 450/2000 | Loss 0.0006 | reward -0.25\n",
            "0.06295250356197357\n",
            "Epoch 500/2000 | Loss 0.0042 | reward 0.95\n",
            "0.0986301526427269\n",
            "Epoch 550/2000 | Loss 0.0012 | reward -0.3\n",
            "0.07480169832706451\n",
            "Epoch 600/2000 | Loss 0.0005 | reward -0.05\n",
            "0.0812639445066452\n",
            "Epoch 650/2000 | Loss 0.0099 | reward 0.75\n",
            "0.12117545306682587\n",
            "Epoch 700/2000 | Loss 0.0331 | reward 0.85\n",
            "0.1761949360370636\n",
            "Epoch 750/2000 | Loss 0.0036 | reward -0.2\n",
            "0.16052378714084625\n",
            "Epoch 800/2000 | Loss 0.0616 | reward -0.8000000000000002\n",
            "0.21829086542129517\n",
            "Epoch 850/2000 | Loss 0.0015 | reward -1.2500000000000004\n",
            "0.2044389843940735\n",
            "Epoch 900/2000 | Loss 0.0008 | reward -1.1000000000000003\n",
            "0.13629820942878723\n",
            "Epoch 950/2000 | Loss 0.0011 | reward -0.2\n",
            "0.14330807328224182\n",
            "Epoch 1000/2000 | Loss 0.0019 | reward -0.1\n",
            "0.1782495081424713\n",
            "Epoch 1050/2000 | Loss 0.0130 | reward 0.04999999999999971\n",
            "0.18583944439888\n",
            "Epoch 1100/2000 | Loss 0.0006 | reward -1.2500000000000004\n",
            "0.2647963762283325\n",
            "Epoch 1150/2000 | Loss 0.0013 | reward 0.6000000000000001\n",
            "0.2526251971721649\n",
            "Epoch 1200/2000 | Loss 0.0111 | reward 0.6000000000000001\n",
            "0.2669517993927002\n",
            "Epoch 1250/2000 | Loss 0.0014 | reward 0.2499999999999999\n",
            "0.25566014647483826\n",
            "Epoch 1300/2000 | Loss 0.0001 | reward 0.04999999999999971\n",
            "0.43581855297088623\n",
            "Epoch 1350/2000 | Loss 0.0004 | reward 0.09999999999999976\n",
            "0.43364375829696655\n",
            "Epoch 1400/2000 | Loss 0.0005 | reward -1.2500000000000004\n",
            "0.5544100999832153\n",
            "Epoch 1450/2000 | Loss 0.0005 | reward -1.2500000000000004\n",
            "0.500420093536377\n",
            "Epoch 1500/2000 | Loss 0.0011 | reward 0.65\n",
            "0.6518521308898926\n",
            "Epoch 1550/2000 | Loss 0.0009 | reward 0.55\n",
            "0.6348155736923218\n",
            "Epoch 1600/2000 | Loss 0.0006 | reward 0.7\n",
            "0.6982742547988892\n",
            "Epoch 1650/2000 | Loss 0.0025 | reward 0.7\n",
            "0.5951724052429199\n",
            "Epoch 1700/2000 | Loss 0.0013 | reward 0.8\n",
            "0.7182576656341553\n",
            "Epoch 1750/2000 | Loss 0.0001 | reward 0.55\n",
            "0.6725962162017822\n",
            "Epoch 1800/2000 | Loss 0.0005 | reward 0.19999999999999984\n",
            "0.660778820514679\n",
            "Epoch 1850/2000 | Loss 0.0005 | reward 0.5\n",
            "0.6887546181678772\n",
            "Epoch 1900/2000 | Loss 0.0009 | reward 0.6000000000000001\n",
            "0.7157112956047058\n",
            "Epoch 1950/2000 | Loss 0.0009 | reward 0.29999999999999993\n",
            "0.6937111616134644\n",
            "epoch 0 : total reward = 0.8\n",
            "epoch 1 : total reward = 0.29999999999999993\n",
            "epoch 2 : total reward = 0.8\n",
            "epoch 3 : total reward = -0.25000000000000044\n",
            "epoch 4 : total reward = 0.8\n",
            "epoch 5 : total reward = 0.8\n",
            "epoch 6 : total reward = -0.25000000000000044\n",
            "epoch 7 : total reward = 0.9\n",
            "epoch 8 : total reward = 0.8\n",
            "epoch 9 : total reward = 0.4\n",
            "epoch 10 : total reward = 0.8\n",
            "epoch 11 : total reward = 0.8\n",
            "epoch 12 : total reward = 0.75\n",
            "epoch 13 : total reward = 0.8\n",
            "epoch 14 : total reward = 0.8\n",
            "epoch 15 : total reward = 0.75\n",
            "epoch 16 : total reward = -0.25000000000000044\n",
            "epoch 17 : total reward = 0.8\n",
            "epoch 18 : total reward = 0.9\n",
            "epoch 19 : total reward = 0.29999999999999993\n",
            "Final score: 0.5775000000000001\n",
            "Epoch 000/3000 | Loss 0.0005 | reward 0.1499999999999998\n",
            "0.6277370452880859\n",
            "Epoch 050/3000 | Loss 0.0004 | reward 0.8\n",
            "0.7232553362846375\n",
            "Epoch 100/3000 | Loss 0.0003 | reward 0.09999999999999976\n",
            "0.6384751796722412\n",
            "Epoch 150/3000 | Loss 0.0012 | reward 0.75\n",
            "0.6965293884277344\n",
            "Epoch 200/3000 | Loss 0.0005 | reward 0.29999999999999993\n",
            "0.5587466359138489\n",
            "Epoch 250/3000 | Loss 0.0015 | reward 0.55\n",
            "0.6585794687271118\n",
            "Epoch 300/3000 | Loss 0.0007 | reward 0.4\n",
            "0.7662710547447205\n",
            "Epoch 350/3000 | Loss 0.0006 | reward 0.1499999999999998\n",
            "0.7088617086410522\n",
            "Epoch 400/3000 | Loss 0.0009 | reward 0.65\n",
            "0.7285902500152588\n",
            "Epoch 450/3000 | Loss 0.0009 | reward 0.8\n",
            "0.6970884203910828\n",
            "Epoch 500/3000 | Loss 0.0003 | reward 0.6000000000000001\n",
            "0.7201694846153259\n",
            "Epoch 550/3000 | Loss 0.0021 | reward 0.65\n",
            "0.7241019010543823\n",
            "Epoch 600/3000 | Loss 0.0005 | reward 0.85\n",
            "0.7117897272109985\n",
            "Epoch 650/3000 | Loss 0.0004 | reward 0.2499999999999999\n",
            "0.6951322555541992\n",
            "Epoch 700/3000 | Loss 0.0002 | reward 0.8\n",
            "0.6977765560150146\n",
            "Epoch 750/3000 | Loss 0.0016 | reward 0.65\n",
            "0.6988271474838257\n",
            "Epoch 800/3000 | Loss 0.0006 | reward -0.25000000000000044\n",
            "0.6831542253494263\n",
            "Epoch 850/3000 | Loss 0.0005 | reward -0.25000000000000044\n",
            "0.7234542369842529\n",
            "Epoch 900/3000 | Loss 0.0006 | reward 0.6000000000000001\n",
            "0.7465320229530334\n",
            "Epoch 950/3000 | Loss 0.0003 | reward 0.75\n",
            "0.7621140480041504\n",
            "Epoch 1000/3000 | Loss 0.0002 | reward 0.7\n",
            "0.7437047362327576\n",
            "Epoch 1050/3000 | Loss 0.0003 | reward 0.85\n",
            "0.6324746608734131\n",
            "Epoch 1100/3000 | Loss 0.0006 | reward 0.65\n",
            "0.6498467922210693\n",
            "Epoch 1150/3000 | Loss 0.0013 | reward -0.25000000000000044\n",
            "0.7488991022109985\n",
            "Epoch 1200/3000 | Loss 0.0003 | reward 0.7\n",
            "0.7809544801712036\n",
            "Epoch 1250/3000 | Loss 0.0008 | reward 0.2499999999999999\n",
            "0.7692406177520752\n",
            "Epoch 1300/3000 | Loss 0.0013 | reward 0.85\n",
            "0.695214033126831\n",
            "Epoch 1350/3000 | Loss 0.0004 | reward -1.2500000000000004\n",
            "0.7405267953872681\n",
            "Epoch 1400/3000 | Loss 0.0004 | reward 0.5\n",
            "0.7247260808944702\n",
            "Epoch 1450/3000 | Loss 0.0004 | reward 0.75\n",
            "0.6920547485351562\n",
            "Epoch 1500/3000 | Loss 0.0005 | reward 0.8\n",
            "0.7220579981803894\n",
            "Epoch 1550/3000 | Loss 0.0005 | reward 0.8\n",
            "0.7719208002090454\n",
            "Epoch 1600/3000 | Loss 0.0005 | reward 0.85\n",
            "0.7708554267883301\n",
            "Epoch 1650/3000 | Loss 0.0002 | reward 0.6000000000000001\n",
            "0.6955111026763916\n",
            "Epoch 1700/3000 | Loss 0.0003 | reward 0.5\n",
            "0.780769944190979\n",
            "Epoch 1750/3000 | Loss 0.0003 | reward 0.8\n",
            "0.6804909110069275\n",
            "Epoch 1800/3000 | Loss 0.0003 | reward 0.75\n",
            "0.6858615875244141\n",
            "Epoch 1850/3000 | Loss 0.0004 | reward 0.6000000000000001\n",
            "0.6550072431564331\n",
            "Epoch 1900/3000 | Loss 0.0005 | reward 0.9\n",
            "0.8068078756332397\n",
            "Epoch 1950/3000 | Loss 0.0006 | reward 0.45000000000000007\n",
            "0.72794508934021\n",
            "Epoch 2000/3000 | Loss 0.0002 | reward 0.6000000000000001\n",
            "0.7440155744552612\n",
            "Epoch 2050/3000 | Loss 0.0002 | reward 0.65\n",
            "0.73860102891922\n",
            "Epoch 2100/3000 | Loss 0.0007 | reward 0.65\n",
            "0.6898196935653687\n",
            "Epoch 2150/3000 | Loss 0.0004 | reward 0.75\n",
            "0.7455358505249023\n",
            "Epoch 2200/3000 | Loss 0.0001 | reward 0.65\n",
            "0.7291449308395386\n",
            "Epoch 2250/3000 | Loss 0.0003 | reward 0.55\n",
            "0.7603869438171387\n",
            "Epoch 2300/3000 | Loss 0.0001 | reward 0.5\n",
            "0.7479865550994873\n",
            "Epoch 2350/3000 | Loss 0.0007 | reward 0.7\n",
            "0.6904452443122864\n",
            "Epoch 2400/3000 | Loss 0.0001 | reward 0.7\n",
            "0.737406849861145\n",
            "Epoch 2450/3000 | Loss 0.0005 | reward 0.8\n",
            "0.7496767044067383\n",
            "Epoch 2500/3000 | Loss 0.0006 | reward 0.5\n",
            "0.7170634269714355\n",
            "Epoch 2550/3000 | Loss 0.0002 | reward 0.8\n",
            "0.727532148361206\n",
            "Epoch 2600/3000 | Loss 0.0002 | reward 0.65\n",
            "0.7444068193435669\n",
            "Epoch 2650/3000 | Loss 0.0003 | reward 0.8\n",
            "0.7547513246536255\n",
            "Epoch 2700/3000 | Loss 0.0006 | reward 0.65\n",
            "0.7868527770042419\n",
            "Epoch 2750/3000 | Loss 0.0004 | reward 0.65\n",
            "0.78116774559021\n",
            "Epoch 2800/3000 | Loss 0.0001 | reward 0.75\n",
            "0.7175098061561584\n",
            "Epoch 2850/3000 | Loss 0.0006 | reward 0.55\n",
            "0.7585421204566956\n",
            "Epoch 2900/3000 | Loss 0.0005 | reward 0.5\n",
            "0.732140839099884\n",
            "Epoch 2950/3000 | Loss 0.0003 | reward 0.8\n",
            "0.7268788814544678\n",
            "epoch 0 : total reward = 0.75\n",
            "epoch 1 : total reward = 0.9\n",
            "epoch 2 : total reward = 0.6000000000000001\n",
            "epoch 3 : total reward = 0.75\n",
            "epoch 4 : total reward = 0.6000000000000001\n",
            "epoch 5 : total reward = 0.6000000000000001\n",
            "epoch 6 : total reward = 0.7\n",
            "epoch 7 : total reward = 0.75\n",
            "epoch 8 : total reward = 0.75\n",
            "epoch 9 : total reward = 0.85\n",
            "epoch 10 : total reward = 0.75\n",
            "epoch 11 : total reward = 0.65\n",
            "epoch 12 : total reward = 0.65\n",
            "epoch 13 : total reward = 0.75\n",
            "epoch 14 : total reward = 0.75\n",
            "epoch 15 : total reward = 0.6000000000000001\n",
            "epoch 16 : total reward = 0.75\n",
            "epoch 17 : total reward = 0.7\n",
            "epoch 18 : total reward = 0.85\n",
            "epoch 19 : total reward = 0.7\n",
            "Final score: 0.72\n",
            "Epoch 000/10000 | Loss 0.0028 | reward 0.75\n",
            "0.6572976112365723\n",
            "Epoch 050/10000 | Loss 0.0017 | reward 0.75\n",
            "0.6657837629318237\n",
            "Epoch 100/10000 | Loss 0.0023 | reward 0.95\n",
            "0.7478041648864746\n",
            "Epoch 150/10000 | Loss 0.0006 | reward 0.5\n",
            "0.7038165330886841\n",
            "Epoch 200/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7743409276008606\n",
            "Epoch 250/10000 | Loss 0.0004 | reward 0.75\n",
            "0.6947898864746094\n",
            "Epoch 300/10000 | Loss 0.0002 | reward 0.8\n",
            "0.774137556552887\n",
            "Epoch 350/10000 | Loss 0.0006 | reward 0.8\n",
            "0.6895776987075806\n",
            "Epoch 400/10000 | Loss 0.0004 | reward 0.75\n",
            "0.7411741018295288\n",
            "Epoch 450/10000 | Loss 0.0008 | reward 0.75\n",
            "0.7604857683181763\n",
            "Epoch 500/10000 | Loss 0.0004 | reward 0.9\n",
            "0.6760842204093933\n",
            "Epoch 550/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7175043821334839\n",
            "Epoch 600/10000 | Loss 0.0005 | reward 0.8\n",
            "0.7444816827774048\n",
            "Epoch 650/10000 | Loss 0.0006 | reward 0.75\n",
            "0.701075553894043\n",
            "Epoch 700/10000 | Loss 0.0001 | reward 0.85\n",
            "0.722315788269043\n",
            "Epoch 750/10000 | Loss 0.0017 | reward 0.85\n",
            "0.738217830657959\n",
            "Epoch 800/10000 | Loss 0.0005 | reward 0.65\n",
            "0.7644928693771362\n",
            "Epoch 850/10000 | Loss 0.0003 | reward 0.85\n",
            "0.7754174470901489\n",
            "Epoch 900/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7342283129692078\n",
            "Epoch 950/10000 | Loss 0.0002 | reward 0.85\n",
            "0.7134326696395874\n",
            "Epoch 1000/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7020412087440491\n",
            "Epoch 1050/10000 | Loss 0.0002 | reward 0.85\n",
            "0.7470697164535522\n",
            "Epoch 1100/10000 | Loss 0.0001 | reward 0.7\n",
            "0.756848156452179\n",
            "Epoch 1150/10000 | Loss 0.0002 | reward 0.5\n",
            "0.781417965888977\n",
            "Epoch 1200/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7216402292251587\n",
            "Epoch 1250/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7646654844284058\n",
            "Epoch 1300/10000 | Loss 0.0005 | reward 0.85\n",
            "0.7227674126625061\n",
            "Epoch 1350/10000 | Loss 0.0002 | reward 0.85\n",
            "0.7377138137817383\n",
            "Epoch 1400/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7459118366241455\n",
            "Epoch 1450/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7841585874557495\n",
            "Epoch 1500/10000 | Loss 0.0020 | reward 0.5\n",
            "0.6288963556289673\n",
            "Epoch 1550/10000 | Loss 0.0016 | reward 0.65\n",
            "0.7394315004348755\n",
            "Epoch 1600/10000 | Loss 0.0003 | reward 0.85\n",
            "0.7855556011199951\n",
            "Epoch 1650/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7148264646530151\n",
            "Epoch 1700/10000 | Loss 0.0004 | reward 0.8\n",
            "0.7442123889923096\n",
            "Epoch 1750/10000 | Loss 0.0007 | reward 0.85\n",
            "0.7400977611541748\n",
            "Epoch 1800/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7059816718101501\n",
            "Epoch 1850/10000 | Loss 0.0006 | reward 0.85\n",
            "0.7486021518707275\n",
            "Epoch 1900/10000 | Loss 0.0003 | reward 0.6000000000000001\n",
            "0.742010235786438\n",
            "Epoch 1950/10000 | Loss 0.0011 | reward 0.8\n",
            "0.6979234218597412\n",
            "Epoch 2000/10000 | Loss 0.0001 | reward 0.8\n",
            "0.8039323091506958\n",
            "Epoch 2050/10000 | Loss 0.0007 | reward 0.9\n",
            "0.7633253931999207\n",
            "Epoch 2100/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7804352045059204\n",
            "Epoch 2150/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7401915788650513\n",
            "Epoch 2200/10000 | Loss 0.0004 | reward 0.7\n",
            "0.7502304315567017\n",
            "Epoch 2250/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7730018496513367\n",
            "Epoch 2300/10000 | Loss 0.0004 | reward 0.6000000000000001\n",
            "0.792036235332489\n",
            "Epoch 2350/10000 | Loss 0.0010 | reward 0.65\n",
            "0.7113193273544312\n",
            "Epoch 2400/10000 | Loss 0.0005 | reward 0.9\n",
            "0.7536333799362183\n",
            "Epoch 2450/10000 | Loss 0.0003 | reward 0.6000000000000001\n",
            "0.7228174209594727\n",
            "Epoch 2500/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7016890048980713\n",
            "Epoch 2550/10000 | Loss 0.0004 | reward 0.6000000000000001\n",
            "0.7524198293685913\n",
            "Epoch 2600/10000 | Loss 0.0001 | reward 0.75\n",
            "0.727306604385376\n",
            "Epoch 2650/10000 | Loss 0.0002 | reward 0.85\n",
            "0.7408144474029541\n",
            "Epoch 2700/10000 | Loss 0.0004 | reward 0.8\n",
            "0.7650179862976074\n",
            "Epoch 2750/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7274961471557617\n",
            "Epoch 2800/10000 | Loss 0.0008 | reward 0.55\n",
            "0.7368483543395996\n",
            "Epoch 2850/10000 | Loss 0.0004 | reward 0.75\n",
            "0.7554792165756226\n",
            "Epoch 2900/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7682385444641113\n",
            "Epoch 2950/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7515696287155151\n",
            "Epoch 3000/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7665348649024963\n",
            "Epoch 3050/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7536256909370422\n",
            "Epoch 3100/10000 | Loss 0.0003 | reward 0.45000000000000007\n",
            "0.7736859321594238\n",
            "Epoch 3150/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7592799663543701\n",
            "Epoch 3200/10000 | Loss 0.0001 | reward 0.7\n",
            "0.6985366940498352\n",
            "Epoch 3250/10000 | Loss 0.0003 | reward 0.7\n",
            "0.7901932001113892\n",
            "Epoch 3300/10000 | Loss 0.0010 | reward 0.75\n",
            "0.7425281405448914\n",
            "Epoch 3350/10000 | Loss 0.0006 | reward 0.7\n",
            "0.7900374531745911\n",
            "Epoch 3400/10000 | Loss 0.0005 | reward 0.75\n",
            "0.7382814288139343\n",
            "Epoch 3450/10000 | Loss 0.0001 | reward 0.8\n",
            "0.750187873840332\n",
            "Epoch 3500/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7175368666648865\n",
            "Epoch 3550/10000 | Loss 0.0003 | reward 0.7\n",
            "0.743335485458374\n",
            "Epoch 3600/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7462257146835327\n",
            "Epoch 3650/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7318402528762817\n",
            "Epoch 3700/10000 | Loss 0.0003 | reward 0.55\n",
            "0.72505784034729\n",
            "Epoch 3750/10000 | Loss 0.0004 | reward 0.75\n",
            "0.74667888879776\n",
            "Epoch 3800/10000 | Loss 0.0003 | reward 0.7\n",
            "0.688845694065094\n",
            "Epoch 3850/10000 | Loss 0.0002 | reward 0.85\n",
            "0.6968719959259033\n",
            "Epoch 3900/10000 | Loss 0.0001 | reward 0.65\n",
            "0.7325770854949951\n",
            "Epoch 3950/10000 | Loss 0.0014 | reward 0.7\n",
            "0.6562191247940063\n",
            "Epoch 4000/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7136644124984741\n",
            "Epoch 4050/10000 | Loss 0.0004 | reward 0.75\n",
            "0.6907054781913757\n",
            "Epoch 4100/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7266535758972168\n",
            "Epoch 4150/10000 | Loss 0.0003 | reward 0.7\n",
            "0.7678663730621338\n",
            "Epoch 4200/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7013217806816101\n",
            "Epoch 4250/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7441413402557373\n",
            "Epoch 4300/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7396215200424194\n",
            "Epoch 4350/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7388545274734497\n",
            "Epoch 4400/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7528952956199646\n",
            "Epoch 4450/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7886819839477539\n",
            "Epoch 4500/10000 | Loss 0.0003 | reward 0.7\n",
            "0.7512744665145874\n",
            "Epoch 4550/10000 | Loss 0.0002 | reward 0.75\n",
            "0.739509105682373\n",
            "Epoch 4600/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7391242384910583\n",
            "Epoch 4650/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7377201318740845\n",
            "Epoch 4700/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7666098475456238\n",
            "Epoch 4750/10000 | Loss 0.0001 | reward 0.7\n",
            "0.728725254535675\n",
            "Epoch 4800/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7359789609909058\n",
            "Epoch 4850/10000 | Loss 0.0002 | reward 0.85\n",
            "0.7705487012863159\n",
            "Epoch 4900/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7246701717376709\n",
            "Epoch 4950/10000 | Loss 0.0003 | reward 0.7\n",
            "0.7309824824333191\n",
            "Epoch 5000/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7279136180877686\n",
            "Epoch 5050/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7410688400268555\n",
            "Epoch 5100/10000 | Loss 0.0008 | reward 0.6000000000000001\n",
            "0.7564550638198853\n",
            "Epoch 5150/10000 | Loss 0.0002 | reward 0.8\n",
            "0.725792407989502\n",
            "Epoch 5200/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7498241066932678\n",
            "Epoch 5250/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7416703104972839\n",
            "Epoch 5300/10000 | Loss 0.0004 | reward 0.4\n",
            "0.7138984203338623\n",
            "Epoch 5350/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7325683236122131\n",
            "Epoch 5400/10000 | Loss 0.0002 | reward 0.8\n",
            "0.721886157989502\n",
            "Epoch 5450/10000 | Loss 0.0003 | reward 0.85\n",
            "0.7191611528396606\n",
            "Epoch 5500/10000 | Loss 0.0004 | reward 0.85\n",
            "0.7475113868713379\n",
            "Epoch 5550/10000 | Loss 0.0005 | reward 0.6000000000000001\n",
            "0.7886375188827515\n",
            "Epoch 5600/10000 | Loss 0.0004 | reward 0.85\n",
            "0.7031201124191284\n",
            "Epoch 5650/10000 | Loss 0.0004 | reward 0.75\n",
            "0.7075051665306091\n",
            "Epoch 5700/10000 | Loss 0.0001 | reward 0.8\n",
            "0.735332190990448\n",
            "Epoch 5750/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7478388547897339\n",
            "Epoch 5800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7748407125473022\n",
            "Epoch 5850/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7471831440925598\n",
            "Epoch 5900/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7483267188072205\n",
            "Epoch 5950/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7474615573883057\n",
            "Epoch 6000/10000 | Loss 0.0004 | reward 0.65\n",
            "0.7518153190612793\n",
            "Epoch 6050/10000 | Loss 0.0003 | reward 0.8\n",
            "0.7393492460250854\n",
            "Epoch 6100/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7318907976150513\n",
            "Epoch 6150/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7285279035568237\n",
            "Epoch 6200/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7204031944274902\n",
            "Epoch 6250/10000 | Loss 0.0001 | reward 0.8\n",
            "0.729777455329895\n",
            "Epoch 6300/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7663140296936035\n",
            "Epoch 6350/10000 | Loss 0.0003 | reward 0.7\n",
            "0.7378743886947632\n",
            "Epoch 6400/10000 | Loss 0.0002 | reward 0.85\n",
            "0.7627200484275818\n",
            "Epoch 6450/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7194820046424866\n",
            "Epoch 6500/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7375932931900024\n",
            "Epoch 6550/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7299658060073853\n",
            "Epoch 6600/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7485129833221436\n",
            "Epoch 6650/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7538480758666992\n",
            "Epoch 6700/10000 | Loss 0.0003 | reward 0.85\n",
            "0.7494877576828003\n",
            "Epoch 6750/10000 | Loss 0.0002 | reward 0.75\n",
            "0.769242525100708\n",
            "Epoch 6800/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7840420603752136\n",
            "Epoch 6850/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7185896635055542\n",
            "Epoch 6900/10000 | Loss 0.0003 | reward 0.7\n",
            "0.7257477641105652\n",
            "Epoch 6950/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7668745517730713\n",
            "Epoch 7000/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7524346709251404\n",
            "Epoch 7050/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7664346098899841\n",
            "Epoch 7100/10000 | Loss 0.0002 | reward 0.85\n",
            "0.7342804670333862\n",
            "Epoch 7150/10000 | Loss 0.0000 | reward 0.7\n",
            "0.7130076885223389\n",
            "Epoch 7200/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7573495507240295\n",
            "Epoch 7250/10000 | Loss 0.0004 | reward 0.29999999999999993\n",
            "0.754475474357605\n",
            "Epoch 7300/10000 | Loss 0.0001 | reward 0.8\n",
            "0.744904637336731\n",
            "Epoch 7350/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7330543398857117\n",
            "Epoch 7400/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7598385810852051\n",
            "Epoch 7450/10000 | Loss 0.0003 | reward 0.55\n",
            "0.7647117376327515\n",
            "Epoch 7500/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7720805406570435\n",
            "Epoch 7550/10000 | Loss 0.0007 | reward 0.6000000000000001\n",
            "0.7648760080337524\n",
            "Epoch 7600/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7438459396362305\n",
            "Epoch 7650/10000 | Loss 0.0004 | reward 0.85\n",
            "0.7461730241775513\n",
            "Epoch 7700/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7334415316581726\n",
            "Epoch 7750/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7171095013618469\n",
            "Epoch 7800/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7486485242843628\n",
            "Epoch 7850/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7367792129516602\n",
            "Epoch 7900/10000 | Loss 0.0004 | reward 0.7\n",
            "0.7807087898254395\n",
            "Epoch 7950/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7513857483863831\n",
            "Epoch 8000/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7337207794189453\n",
            "Epoch 8050/10000 | Loss 0.0001 | reward 0.55\n",
            "0.7178869247436523\n",
            "Epoch 8100/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7301954030990601\n",
            "Epoch 8150/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7587188482284546\n",
            "Epoch 8200/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7069419622421265\n",
            "Epoch 8250/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7341296672821045\n",
            "Epoch 8300/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7318397164344788\n",
            "Epoch 8350/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7472172975540161\n",
            "Epoch 8400/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7609199285507202\n",
            "Epoch 8450/10000 | Loss 0.0004 | reward 0.8\n",
            "0.7459874153137207\n",
            "Epoch 8500/10000 | Loss 0.0024 | reward 0.75\n",
            "0.7388980984687805\n",
            "Epoch 8550/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7485728859901428\n",
            "Epoch 8600/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7373143434524536\n",
            "Epoch 8650/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7746225595474243\n",
            "Epoch 8700/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7451233863830566\n",
            "Epoch 8750/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7380900382995605\n",
            "Epoch 8800/10000 | Loss 0.0003 | reward 0.85\n",
            "0.7499807476997375\n",
            "Epoch 8850/10000 | Loss 0.0003 | reward 0.75\n",
            "0.72413170337677\n",
            "Epoch 8900/10000 | Loss 0.0003 | reward 0.8\n",
            "0.7761509418487549\n",
            "Epoch 8950/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7179199457168579\n",
            "Epoch 9000/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7203267812728882\n",
            "Epoch 9050/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7199031114578247\n",
            "Epoch 9100/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7748792171478271\n",
            "Epoch 9150/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7350579500198364\n",
            "Epoch 9200/10000 | Loss 0.0001 | reward 0.8\n",
            "0.6867716312408447\n",
            "Epoch 9250/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7154315114021301\n",
            "Epoch 9300/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7436927556991577\n",
            "Epoch 9350/10000 | Loss 0.0001 | reward 0.85\n",
            "0.75543212890625\n",
            "Epoch 9400/10000 | Loss 0.0002 | reward 0.75\n",
            "0.6765953302383423\n",
            "Epoch 9450/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7352167963981628\n",
            "Epoch 9500/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7457324266433716\n",
            "Epoch 9550/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7433183193206787\n",
            "Epoch 9600/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7071805000305176\n",
            "Epoch 9650/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7396077513694763\n",
            "Epoch 9700/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7242562770843506\n",
            "Epoch 9750/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7400796413421631\n",
            "Epoch 9800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7005321979522705\n",
            "Epoch 9850/10000 | Loss 0.0000 | reward 0.85\n",
            "0.7066651582717896\n",
            "Epoch 9900/10000 | Loss 0.0001 | reward 0.8\n",
            "0.730771541595459\n",
            "Epoch 9950/10000 | Loss 0.0005 | reward 0.75\n",
            "0.7614912986755371\n",
            "epoch 0 : total reward = 0.75\n",
            "epoch 1 : total reward = 0.8\n",
            "epoch 2 : total reward = 0.8\n",
            "epoch 3 : total reward = 0.75\n",
            "epoch 4 : total reward = 0.8\n",
            "epoch 5 : total reward = 0.85\n",
            "epoch 6 : total reward = 0.85\n",
            "epoch 7 : total reward = 0.8\n",
            "epoch 8 : total reward = 0.8\n",
            "epoch 9 : total reward = 0.85\n",
            "epoch 10 : total reward = 0.75\n",
            "epoch 11 : total reward = 0.75\n",
            "epoch 12 : total reward = 0.75\n",
            "epoch 13 : total reward = 0.8\n",
            "epoch 14 : total reward = 0.8\n",
            "epoch 15 : total reward = 0.85\n",
            "epoch 16 : total reward = 0.8\n",
            "epoch 17 : total reward = 0.75\n",
            "epoch 18 : total reward = 0.75\n",
            "epoch 19 : total reward = 0.85\n",
            "Final score: 0.795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QvwXFkZmxIQs",
        "colab_type": "code",
        "outputId": "e6f78dd1-e939-47e8-d055-d6f10cfefeff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7211
        }
      },
      "cell_type": "code",
      "source": [
        "agent = DQN_separated_net(segment_length=SEGMENT_LENGTH, epsilon=0.1, memory_size=memory_size, batch_size=32)\n",
        "train(agent,env,10000,prefix='DQN15000_to_25000', next_after_e=True, draw=False)\n",
        "test(agent,env,20,prefix='DQN15000_to_25000', next_after_e=True, draw=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 000/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7571251392364502\n",
            "Epoch 050/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7503526210784912\n",
            "Epoch 100/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7459627985954285\n",
            "Epoch 150/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7430984973907471\n",
            "Epoch 200/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7399482727050781\n",
            "Epoch 250/10000 | Loss 0.0002 | reward 0.75\n",
            "0.754023551940918\n",
            "Epoch 300/10000 | Loss 0.0001 | reward 0.65\n",
            "0.7372941970825195\n",
            "Epoch 350/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7458965182304382\n",
            "Epoch 400/10000 | Loss 0.0005 | reward 0.65\n",
            "0.7467712163925171\n",
            "Epoch 450/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7244037985801697\n",
            "Epoch 500/10000 | Loss 0.0003 | reward 0.7\n",
            "0.7393524646759033\n",
            "Epoch 550/10000 | Loss 0.0001 | reward 0.8\n",
            "0.755128026008606\n",
            "Epoch 600/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7556039094924927\n",
            "Epoch 650/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7836644649505615\n",
            "Epoch 700/10000 | Loss 0.0002 | reward 0.65\n",
            "0.6995659470558167\n",
            "Epoch 750/10000 | Loss 0.0001 | reward 0.75\n",
            "0.741199254989624\n",
            "Epoch 800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7443358302116394\n",
            "Epoch 850/10000 | Loss 0.0001 | reward 0.75\n",
            "0.761055588722229\n",
            "Epoch 900/10000 | Loss 0.0005 | reward 0.65\n",
            "0.7209353446960449\n",
            "Epoch 950/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7271915078163147\n",
            "Epoch 1000/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7321171760559082\n",
            "Epoch 1050/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7421165704727173\n",
            "Epoch 1100/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7305363416671753\n",
            "Epoch 1150/10000 | Loss 0.0005 | reward 0.45000000000000007\n",
            "0.7184270620346069\n",
            "Epoch 1200/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7486903667449951\n",
            "Epoch 1250/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7659593224525452\n",
            "Epoch 1300/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7508878707885742\n",
            "Epoch 1350/10000 | Loss 0.0002 | reward 0.7\n",
            "0.6972476840019226\n",
            "Epoch 1400/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7665154337882996\n",
            "Epoch 1450/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7477805614471436\n",
            "Epoch 1500/10000 | Loss 0.0001 | reward 0.85\n",
            "0.798442006111145\n",
            "Epoch 1550/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7417621612548828\n",
            "Epoch 1600/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7260881662368774\n",
            "Epoch 1650/10000 | Loss 0.0001 | reward 0.8\n",
            "0.753824770450592\n",
            "Epoch 1700/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7681117057800293\n",
            "Epoch 1750/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7633103132247925\n",
            "Epoch 1800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7505645155906677\n",
            "Epoch 1850/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7856669425964355\n",
            "Epoch 1900/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7416554689407349\n",
            "Epoch 1950/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7747948169708252\n",
            "Epoch 2000/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7584434747695923\n",
            "Epoch 2050/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7506189346313477\n",
            "Epoch 2100/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7443190217018127\n",
            "Epoch 2150/10000 | Loss 0.0002 | reward 0.65\n",
            "0.747799813747406\n",
            "Epoch 2200/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7488264441490173\n",
            "Epoch 2250/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7486060857772827\n",
            "Epoch 2300/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7563644647598267\n",
            "Epoch 2350/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7404506206512451\n",
            "Epoch 2400/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7868484854698181\n",
            "Epoch 2450/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7831846475601196\n",
            "Epoch 2500/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7866420745849609\n",
            "Epoch 2550/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7612617015838623\n",
            "Epoch 2600/10000 | Loss 0.0003 | reward 0.8\n",
            "0.7583410143852234\n",
            "Epoch 2650/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7496894001960754\n",
            "Epoch 2700/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7556774020195007\n",
            "Epoch 2750/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7585408687591553\n",
            "Epoch 2800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7401333451271057\n",
            "Epoch 2850/10000 | Loss 0.0005 | reward 0.9\n",
            "0.7870805859565735\n",
            "Epoch 2900/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7465217709541321\n",
            "Epoch 2950/10000 | Loss 0.0006 | reward 0.75\n",
            "0.7428817749023438\n",
            "Epoch 3000/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7715404033660889\n",
            "Epoch 3050/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7465870380401611\n",
            "Epoch 3100/10000 | Loss 0.0001 | reward 0.8\n",
            "0.742777407169342\n",
            "Epoch 3150/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7007884979248047\n",
            "Epoch 3200/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7158654928207397\n",
            "Epoch 3250/10000 | Loss 0.0001 | reward 0.65\n",
            "0.7748851776123047\n",
            "Epoch 3300/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7435961365699768\n",
            "Epoch 3350/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7473887205123901\n",
            "Epoch 3400/10000 | Loss 0.0001 | reward 0.65\n",
            "0.7640069127082825\n",
            "Epoch 3450/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7378683090209961\n",
            "Epoch 3500/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7603785991668701\n",
            "Epoch 3550/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7806936502456665\n",
            "Epoch 3600/10000 | Loss 0.0002 | reward 0.75\n",
            "0.779721200466156\n",
            "Epoch 3650/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7731927633285522\n",
            "Epoch 3700/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7401893734931946\n",
            "Epoch 3750/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7634825110435486\n",
            "Epoch 3800/10000 | Loss 0.0002 | reward 0.8\n",
            "0.7528153657913208\n",
            "Epoch 3850/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7471506595611572\n",
            "Epoch 3900/10000 | Loss 0.0002 | reward 0.6000000000000001\n",
            "0.7703258991241455\n",
            "Epoch 3950/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7146974802017212\n",
            "Epoch 4000/10000 | Loss 0.0003 | reward 0.75\n",
            "0.760696530342102\n",
            "Epoch 4050/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7398854494094849\n",
            "Epoch 4100/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7266908884048462\n",
            "Epoch 4150/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7157107591629028\n",
            "Epoch 4200/10000 | Loss 0.0007 | reward 0.7\n",
            "0.764964759349823\n",
            "Epoch 4250/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7639507055282593\n",
            "Epoch 4300/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7386019229888916\n",
            "Epoch 4350/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7617863416671753\n",
            "Epoch 4400/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7451046705245972\n",
            "Epoch 4450/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7594150900840759\n",
            "Epoch 4500/10000 | Loss 0.0001 | reward 0.65\n",
            "0.7377103567123413\n",
            "Epoch 4550/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7222579717636108\n",
            "Epoch 4600/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7512208223342896\n",
            "Epoch 4650/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7262110710144043\n",
            "Epoch 4700/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7519608736038208\n",
            "Epoch 4750/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7588834762573242\n",
            "Epoch 4800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7583440542221069\n",
            "Epoch 4850/10000 | Loss 0.0002 | reward 0.65\n",
            "0.762858510017395\n",
            "Epoch 4900/10000 | Loss 0.0000 | reward 0.65\n",
            "0.7049142718315125\n",
            "Epoch 4950/10000 | Loss 0.0001 | reward 0.75\n",
            "0.752001166343689\n",
            "Epoch 5000/10000 | Loss 0.0003 | reward 0.8\n",
            "0.7327447533607483\n",
            "Epoch 5050/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7496361136436462\n",
            "Epoch 5100/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7532141208648682\n",
            "Epoch 5150/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7414835691452026\n",
            "Epoch 5200/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7526441812515259\n",
            "Epoch 5250/10000 | Loss 0.0001 | reward 0.8\n",
            "0.755885124206543\n",
            "Epoch 5300/10000 | Loss 0.0000 | reward 0.8\n",
            "0.744678258895874\n",
            "Epoch 5350/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7564905881881714\n",
            "Epoch 5400/10000 | Loss 0.0003 | reward 0.65\n",
            "0.7652041912078857\n",
            "Epoch 5450/10000 | Loss 0.0014 | reward -0.25000000000000044\n",
            "0.25071799755096436\n",
            "Epoch 5500/10000 | Loss 0.0013 | reward 0.19999999999999984\n",
            "0.5643644332885742\n",
            "Epoch 5550/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7713282108306885\n",
            "Epoch 5600/10000 | Loss 0.0003 | reward 0.8\n",
            "0.6946582794189453\n",
            "Epoch 5650/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7115049362182617\n",
            "Epoch 5700/10000 | Loss 0.0004 | reward 0.75\n",
            "0.7638722658157349\n",
            "Epoch 5750/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7423070669174194\n",
            "Epoch 5800/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7236206531524658\n",
            "Epoch 5850/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7617205381393433\n",
            "Epoch 5900/10000 | Loss 0.0003 | reward 0.85\n",
            "0.7767002582550049\n",
            "Epoch 5950/10000 | Loss 0.0003 | reward 0.8\n",
            "0.7569355368614197\n",
            "Epoch 6000/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7487705945968628\n",
            "Epoch 6050/10000 | Loss 0.0001 | reward 0.65\n",
            "0.7716274857521057\n",
            "Epoch 6100/10000 | Loss 0.0001 | reward 0.8\n",
            "0.776204526424408\n",
            "Epoch 6150/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7321878671646118\n",
            "Epoch 6200/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7892167568206787\n",
            "Epoch 6250/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7568429708480835\n",
            "Epoch 6300/10000 | Loss 0.0001 | reward 0.8\n",
            "0.745353102684021\n",
            "Epoch 6350/10000 | Loss 0.0003 | reward 0.85\n",
            "0.7864371538162231\n",
            "Epoch 6400/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7338470220565796\n",
            "Epoch 6450/10000 | Loss 0.0001 | reward 0.75\n",
            "0.752144455909729\n",
            "Epoch 6500/10000 | Loss 0.0000 | reward 0.8\n",
            "0.718591570854187\n",
            "Epoch 6550/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7458561658859253\n",
            "Epoch 6600/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7835332751274109\n",
            "Epoch 6650/10000 | Loss 0.0001 | reward 0.6000000000000001\n",
            "0.7516718506813049\n",
            "Epoch 6700/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7607706785202026\n",
            "Epoch 6750/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7509228587150574\n",
            "Epoch 6800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7497204542160034\n",
            "Epoch 6850/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7384541034698486\n",
            "Epoch 6900/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7954400777816772\n",
            "Epoch 6950/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7523820400238037\n",
            "Epoch 7000/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7457324862480164\n",
            "Epoch 7050/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7320356369018555\n",
            "Epoch 7100/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7636860609054565\n",
            "Epoch 7150/10000 | Loss 0.0000 | reward 0.8\n",
            "0.739889919757843\n",
            "Epoch 7200/10000 | Loss 0.0002 | reward 0.75\n",
            "0.753775954246521\n",
            "Epoch 7250/10000 | Loss 0.0001 | reward 0.75\n",
            "0.753209114074707\n",
            "Epoch 7300/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7695733904838562\n",
            "Epoch 7350/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7790782451629639\n",
            "Epoch 7400/10000 | Loss 0.0004 | reward 0.75\n",
            "0.762344241142273\n",
            "Epoch 7450/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7416313290596008\n",
            "Epoch 7500/10000 | Loss 0.0003 | reward 0.75\n",
            "0.7791221737861633\n",
            "Epoch 7550/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7855346202850342\n",
            "Epoch 7600/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7398080825805664\n",
            "Epoch 7650/10000 | Loss 0.0000 | reward 0.75\n",
            "0.758670449256897\n",
            "Epoch 7700/10000 | Loss 0.0002 | reward 0.7\n",
            "0.7673001885414124\n",
            "Epoch 7750/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7416175007820129\n",
            "Epoch 7800/10000 | Loss 0.0002 | reward 0.65\n",
            "0.7409595847129822\n",
            "Epoch 7850/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7567565441131592\n",
            "Epoch 7900/10000 | Loss 0.0001 | reward 0.75\n",
            "0.758863091468811\n",
            "Epoch 7950/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7276817560195923\n",
            "Epoch 8000/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7070265412330627\n",
            "Epoch 8050/10000 | Loss 0.0005 | reward 0.85\n",
            "0.7552080154418945\n",
            "Epoch 8100/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7645177841186523\n",
            "Epoch 8150/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7428464293479919\n",
            "Epoch 8200/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7257741689682007\n",
            "Epoch 8250/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7450788617134094\n",
            "Epoch 8300/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7746501564979553\n",
            "Epoch 8350/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7558113932609558\n",
            "Epoch 8400/10000 | Loss 0.0001 | reward 0.65\n",
            "0.7398500442504883\n",
            "Epoch 8450/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7669833898544312\n",
            "Epoch 8500/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7432318925857544\n",
            "Epoch 8550/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7343275547027588\n",
            "Epoch 8600/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7497198581695557\n",
            "Epoch 8650/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7315220236778259\n",
            "Epoch 8700/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7809728384017944\n",
            "Epoch 8750/10000 | Loss 0.0001 | reward 0.85\n",
            "0.8011605143547058\n",
            "Epoch 8800/10000 | Loss 0.0002 | reward 0.7\n",
            "0.6934075355529785\n",
            "Epoch 8850/10000 | Loss 0.0002 | reward 0.7\n",
            "0.8023891448974609\n",
            "Epoch 8900/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7745962142944336\n",
            "Epoch 8950/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7838652729988098\n",
            "Epoch 9000/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7331472635269165\n",
            "Epoch 9050/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7465057373046875\n",
            "Epoch 9100/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7436638474464417\n",
            "Epoch 9150/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7452852725982666\n",
            "Epoch 9200/10000 | Loss 0.0001 | reward 0.85\n",
            "0.7405799031257629\n",
            "Epoch 9250/10000 | Loss 0.0002 | reward 0.8\n",
            "0.761760950088501\n",
            "Epoch 9300/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7877371311187744\n",
            "Epoch 9350/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7731462717056274\n",
            "Epoch 9400/10000 | Loss 0.0002 | reward 0.75\n",
            "0.7521350383758545\n",
            "Epoch 9450/10000 | Loss 0.0000 | reward 0.8\n",
            "0.7262910604476929\n",
            "Epoch 9500/10000 | Loss 0.0000 | reward 0.7\n",
            "0.7347817420959473\n",
            "Epoch 9550/10000 | Loss 0.0001 | reward 0.75\n",
            "0.756020724773407\n",
            "Epoch 9600/10000 | Loss 0.0003 | reward 0.8\n",
            "0.7281826734542847\n",
            "Epoch 9650/10000 | Loss 0.0000 | reward 0.7\n",
            "0.7216814756393433\n",
            "Epoch 9700/10000 | Loss 0.0001 | reward 0.7\n",
            "0.7668968439102173\n",
            "Epoch 9750/10000 | Loss 0.0001 | reward 0.8\n",
            "0.7484750747680664\n",
            "Epoch 9800/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7562376856803894\n",
            "Epoch 9850/10000 | Loss 0.0004 | reward 0.75\n",
            "0.7550778388977051\n",
            "Epoch 9900/10000 | Loss 0.0001 | reward 0.75\n",
            "0.7692474126815796\n",
            "Epoch 9950/10000 | Loss 0.0000 | reward 0.75\n",
            "0.7534105777740479\n",
            "epoch 0 : total reward = 0.8\n",
            "epoch 1 : total reward = 0.8\n",
            "epoch 2 : total reward = 0.8\n",
            "epoch 3 : total reward = 0.8\n",
            "epoch 4 : total reward = 0.8\n",
            "epoch 5 : total reward = 0.8\n",
            "epoch 6 : total reward = 0.75\n",
            "epoch 7 : total reward = 0.8\n",
            "epoch 8 : total reward = 0.8\n",
            "epoch 9 : total reward = 0.8\n",
            "epoch 10 : total reward = 0.8\n",
            "epoch 11 : total reward = 0.8\n",
            "epoch 12 : total reward = 0.8\n",
            "epoch 13 : total reward = 0.75\n",
            "epoch 14 : total reward = 0.75\n",
            "epoch 15 : total reward = 0.8\n",
            "epoch 16 : total reward = 0.8\n",
            "epoch 17 : total reward = 0.8\n",
            "epoch 18 : total reward = 0.8\n",
            "epoch 19 : total reward = 0.8\n",
            "Final score: 0.7925000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}