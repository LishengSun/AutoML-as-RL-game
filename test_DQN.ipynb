{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduce Natural Image Experiments using DQN\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from distributions import Categorical, DiagGaussian\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "import img_env \n",
    "\n",
    "import utils\n",
    "\n",
    "import model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from random import randint\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model\n",
    "-------------\n",
    "I keep almost the same architecture as the original code (model.py), except:\n",
    "* I expose Q values (=dist.logits) and classification probability (=clf.logits) to ease the computation of Q values and losses in optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_average(x, factor=10):\n",
    "    running_x = 0\n",
    "    for i in range(len(x)):\n",
    "        U = 1. / min(i+1, factor)\n",
    "        running_x = running_x * (1 - U) + x[i] * U\n",
    "        x[i] = running_x\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class myNet(nn.Module):\n",
    "\tdef __init__(self, obs_shape, action_space, recurrent_policy=False, dataset=None, resnet=False, pretrained=False):\n",
    "\t\tsuper(myNet, self).__init__()\n",
    "\t\tself.dataset = dataset\n",
    "\t\tif len(obs_shape) == 3: #our mnist case\n",
    "\t\t\tself.base = model.CNNBase(obs_shape[0], recurrent_policy, dataset=dataset)\n",
    "\t\telif len(obs_shape) == 1:\n",
    "\t\t\tassert not recurrent_policy, \\\n",
    "\t\t\t\t\"Recurrent policy is not implemented for the MLP controller\"\n",
    "\t\t\tself.base = MLPBase(obs_shape[0])\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif action_space.__class__.__name__ == \"Discrete\": # our case\n",
    "\t\t\tnum_outputs = action_space.n\n",
    "\t\t\tself.dist = Categorical(self.base.output_size, num_outputs)\n",
    "\t\telif action_space.__class__.__name__ == \"Box\":\n",
    "\t\t\tnum_outputs = action_space.shape[0]\n",
    "\t\t\tself.dist = DiagGaussian(self.base.output_size, num_outputs)\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif dataset in ['mnist', 'cifar10']:\n",
    "\t\t\tself.clf = Categorical(self.base.output_size, 2)#10)\n",
    "\n",
    "\t\tself.state_size = self.base.state_size\n",
    "\n",
    "\tdef forward(self, inputs, states, masks):\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef act(self, inputs, states, masks, deterministic=False):\n",
    "\t\tactor_features, states = self.base(inputs, states, masks)\n",
    "\t\tself.actor_features = actor_features\n",
    "\t\tdist = self.dist(actor_features)\n",
    "\t\tQ_values = dist.logits\n",
    "\t\tif deterministic:\n",
    "\t\t\taction = dist.mode()\n",
    "\t\telse:\n",
    "\t\t\taction = dist.sample()\n",
    "\n",
    "\t\taction_log_probs = dist.log_probs(action)\n",
    "\t\tif self.dataset in img_env.IMG_ENVS:\n",
    "\t\t\tclf = self.clf(self.actor_features)\n",
    "\t\t\tclf_proba = clf.logits\n",
    "\t\t\tif deterministic:\n",
    "\t\t\t\tclassif = clf.mode()\n",
    "\t\t\telse:\n",
    "\t\t\t\tclassif = clf.sample()\n",
    "\t\t\taction = torch.cat([action, classif], 1)\n",
    "\t\t\taction_log_probs += clf.log_probs(classif)\n",
    "\n",
    "\t\treturn action, Q_values, clf_proba, action_log_probs, states #dist.logits = Q values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "The code of ReplayMemory is essentially from the [DQN tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "\tdef __init__(self, capacity):\n",
    "\t\tself.capacity = capacity\n",
    "\t\tself.memory = []\n",
    "\t\tself.position = 0\n",
    "\n",
    "\tdef push(self, *args):\n",
    "\t\t\"\"\"Saves a transition.\"\"\"\n",
    "\t\tif len(self.memory) < self.capacity:\n",
    "\t\t\tself.memory.append(None)\n",
    "\t\tself.memory[self.position] = Transition(*args)\n",
    "\t\tself.position = (self.position + 1) % self.capacity\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\treturn random.sample(self.memory, batch_size)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.memory)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "\t\t\t\t\t\t('state', 'action', 'next_state', 'reward', 'curr_label'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "-------------\n",
    "The code of optimization is inspired by the [DQN tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html).             \n",
    "\n",
    "We combine the 2 losses (classification and navigation) to the total loss and optimize the total loss.\n",
    "\n",
    "Note: For the 1-step setting, we train the classification head b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_myNet(net, curr_label, optimizer, BATCH_SIZE=128, optimize_clf=False):\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\ttransitions = memory.sample(BATCH_SIZE)\n",
    "\tbatch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "\tnon_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "\t\t\t\t\t\t\t\t\t\t  batch.next_state)), dtype=torch.uint8).to(device)\n",
    "\n",
    "\tnon_final_next_states = torch.stack([s for s in batch.next_state \\\n",
    "\t\tif s is not None]).to(device)\n",
    "\t# print ('non_final_next_states', non_final_next_states.shape)\n",
    "\tstate_batch = torch.stack(batch.state).to(device)\n",
    "\t# print ('state_batch.size', state_batch.size)\n",
    "\taction_batch = torch.stack(batch.action).to(device)\n",
    "\treward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "\n",
    "\t_, Q_values_batch, clf_proba_batch, _, _ = net.act(\n",
    "\t\tinputs=state_batch.float(),\n",
    "\t\tstates=state_batch, masks=state_batch[1])\n",
    "\n",
    "\tstate_action_values = Q_values_batch.gather(1, action_batch[:, 0].view(BATCH_SIZE,1))\n",
    "\tnext_state_values = torch.zeros(BATCH_SIZE).to(device)\n",
    "\n",
    "\t_, next_Q_values_batch, _, _, _= target_net.act(inputs=non_final_next_states.float(),states=non_final_next_states, masks=non_final_next_states[1])\n",
    "\n",
    "\tnext_state_values[non_final_mask] = next_Q_values_batch.max(1)[0].detach()\n",
    "\n",
    "\texpected_state_action_values = (next_state_values * GAMMA) + reward_batch # Compute the expected Q values\n",
    "\tloss_dist = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\tcurr_label_batch = torch.cat(batch.curr_label).to(device)\n",
    "\tloss_clf = F.nll_loss(clf_proba_batch, curr_label_batch)\n",
    "\n",
    "\ttotal_loss = loss_dist + loss_clf\n",
    "# \toptimizer_dist = optim.RMSprop(net.parameters())\n",
    "# \toptimizer_dist.zero_grad()\n",
    "# \ttotal_loss.backward()\n",
    "# \tfor param in net.dist.parameters():\n",
    "# \t\tparam.grad.data.clamp_(-1, 1)\n",
    "# \toptimizer_dist.step()\n",
    "\toptimizer.zero_grad()\n",
    "\ttotal_loss.backward()\n",
    "\tfor param in net.parameters():\n",
    "\t\tparam.grad.data.clamp_(-1, 1)\n",
    "\toptimizer.step()\n",
    "\n",
    "\treturn total_loss, loss_clf, loss_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ....\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a1631242e8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mdurations_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_durations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#       plt.plot(smoothing_average(durations_t.numpy()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_durations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdurations_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m68\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/seaborn/timeseries.py\u001b[0m in \u001b[0;36mtsplot\u001b[0;34m(data, time, unit, condition, value, err_style, ci, interpolate, color, estimator, n_boot, err_palette, err_kws, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m                                  \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                                  \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                                  cond=conds))\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;31m# Set up the err_style and ci arguments for the loop below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py36/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tBATCH_SIZE = 128\n",
    "\tNUM_STEPS = 10\n",
    "\tGAMMA = 1 - (1 / NUM_STEPS) # Set to horizon of max episode length\n",
    "\tEPS = 0.05\n",
    "\tNUM_LABELS = 2\n",
    "\tWINDOW_SIZE = 14\n",
    "\tNUM_EPISODES = 10\n",
    "\tTARGET_UPDATE = 10\n",
    "\tRUNS = 3\n",
    "\n",
    "\tenv = img_env.ImgEnv('mnist', train=True, max_steps=NUM_STEPS, channels=2, window=WINDOW_SIZE, num_labels=NUM_LABELS)\n",
    "\t\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\trun_durations = []\n",
    "\trun_total_rewards = []\n",
    "\trun_loss_clf = []\n",
    "\tfor run in range(RUNS):\n",
    "\t\tnet = myNet(\\\n",
    "\t\t\tobs_shape=env.observation_space.shape, \\\n",
    "\t\t\taction_space=env.action_space, dataset='mnist').to(device)\n",
    "\t\ttarget_net = myNet(\\\n",
    "\t\t\tobs_shape=env.observation_space.shape, \\\n",
    "\t\t\taction_space=env.action_space, dataset='mnist').to(device)\n",
    "\t\ttarget_net.load_state_dict(net.state_dict())\n",
    "\t\ttarget_net.eval()\n",
    "\t\tmemory = ReplayMemory(10000)\n",
    "\n",
    "\t\ttotal_rewards = []\n",
    "\t\tepisode_durations = []\n",
    "\t\tloss_classification = []\n",
    "\t\tq_values = []\n",
    "\t\toptimizer_clf = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\t\tfor i_episode in range(NUM_EPISODES):\n",
    "\t\t\ttotal_reward_i = 0\n",
    "\t\t\tobservation = env.reset()\n",
    "\t\t\tcurr_label = env.curr_label.item()\n",
    "\t\t\tfor t in range(NUM_STEPS): # allow 100 steps\n",
    "\t\t\t\tactionS, Q_values, clf_proba, action_log_probs, states = net.act(inputs=torch.from_numpy(observation).float().resize_(1, 2, 32, 32).to(device), \\\n",
    "\t\t\t\t\tstates=observation, masks=observation[1])\n",
    "\t\t\t\tactionS = actionS.cpu().numpy()[0]\n",
    "\t\t\t\tclass_pred = actionS[1]\n",
    "\t\t\t\tlast_observation = observation\n",
    "\t\t\t\trand = np.random.rand()\n",
    "\t\t\t\tif rand < EPS:\n",
    "\t\t\t\t\tactionS = np.array(\n",
    "\t\t\t\t\t\t[np.random.choice(range(4)), np.random.choice(range(NUM_LABELS))])\n",
    "\t\t\t\taction = actionS[0]\n",
    "\t\t\t\tobservation, reward, done, info = env.step(actionS)\n",
    "\t\t\t\ttotal_reward_i = reward + GAMMA*total_reward_i\n",
    "\t\t\t\tmemory.push(torch.from_numpy(last_observation), torch.from_numpy(actionS), \\\n",
    "\t\t\t\t\ttorch.from_numpy(observation), torch.tensor([reward]).float(), torch.tensor([curr_label]))\n",
    "\t\t\t\toptimize_myNet(net, curr_label, optimizer_clf, BATCH_SIZE)\n",
    "\n",
    "\t\t\t\tif done:\n",
    "\t\t\t\t# print ('Done after %i steps'%(t+1))\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t# Update the target network, copying all weights and biases in DQN\n",
    "\t\t\tif i_episode % TARGET_UPDATE == 0:\n",
    "\t\t\t\ttarget_net.load_state_dict(net.state_dict())\n",
    "\n",
    "\t\t\tloss_classification_i = F.nll_loss(clf_proba, env.curr_label.unsqueeze_(dim=0).to(device))\n",
    "\t\t\ttotal_rewards.append(total_reward_i)\n",
    "\t\t\tepisode_durations.append(t)\n",
    "\t\t\tloss_classification.append(loss_classification_i)\n",
    "\t\t\tq_values.append(Q_values)\n",
    "\t\trun_durations.append(episode_durations)\n",
    "\t\trun_total_rewards.append(total_rewards)\n",
    "\t\trun_loss_clf.append(loss_classification)\n",
    "        \n",
    "\tplt.title('Class 0')\n",
    "\tplt.subplot(3, 1, 1)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Episode_Duration')\n",
    "\tdurations_t = torch.tensor(episode_durations[0], dtype=torch.float)\n",
    "# \tplt.plot(smoothing_average(durations_t.numpy()))\n",
    "\tsns.tsplot(data=run_durations, time=list(range(NUM_EPISODE, ci=[68, 95], ax=plt.subplot(3, 1, 1), color='red')\n",
    "\t\t\n",
    "\tplt.subplot(3, 1, 2)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Rewards')\n",
    "\ttotal_rewards_t = torch.tensor(total_rewards[0], dtype=torch.float)\n",
    "# \tplt.plot(smoothing_average(total_rewards_t.numpy()))\n",
    "\tsns.tsplot(data=run_total_rewards, time=np.array(range(len(durations_t))), ci=[68, 95], ax=plt.subplot(3, 1, 2), color='red')\n",
    "\t\t\n",
    "\tplt.subplot(3, 1, 3)\n",
    "\tplt.ylim(top=1)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Loss Classification')\n",
    "\tloss_classification_t = torch.tensor(loss_classification[0], dtype=torch.float)\n",
    "# \tplt.plot(smoothing_average(loss_classification_t.numpy()))\n",
    "\tsns.tsplot(data=run_loss_clf, time=np.array(range(len(durations_t))), ci=[68, 95], ax=plt.subplot(3, 1, 3), color='red')\n",
    "\n",
    "\tplt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
