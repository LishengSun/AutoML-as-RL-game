{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduce Natural Image Experiments using DQN\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from distributions import Categorical, DiagGaussian\n",
    "from collections import namedtuple\n",
    "\n",
    "import img_env \n",
    "\n",
    "import utils\n",
    "\n",
    "import model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from random import randint\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model\n",
    "-------------\n",
    "I keep almost the same architecture as the original code (model.py), except:\n",
    "* I expose Q values (=dist.logits) and classification probability (=clf.logits) to ease the computation of Q values and losses in optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_average(x, factor=10):\n",
    "    running_x = 0\n",
    "    for i in range(len(x)):\n",
    "        U = 1. / min(i+1, factor)\n",
    "        running_x = running_x * (1 - U) + x[i] * U\n",
    "        x[i] = running_x\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class myNet(nn.Module):\n",
    "\tdef __init__(self, obs_shape, action_space, recurrent_policy=False, dataset=None, resnet=False, pretrained=False):\n",
    "\t\tsuper(myNet, self).__init__()\n",
    "\t\tself.dataset = dataset\n",
    "\t\tif len(obs_shape) == 3: #our mnist case\n",
    "\t\t\tself.base = model.CNNBase(obs_shape[0], recurrent_policy, dataset=dataset)\n",
    "\t\telif len(obs_shape) == 1:\n",
    "\t\t\tassert not recurrent_policy, \\\n",
    "\t\t\t\t\"Recurrent policy is not implemented for the MLP controller\"\n",
    "\t\t\tself.base = MLPBase(obs_shape[0])\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif action_space.__class__.__name__ == \"Discrete\": # our case\n",
    "\t\t\tnum_outputs = action_space.n\n",
    "\t\t\tself.dist = Categorical(self.base.output_size, num_outputs)\n",
    "\t\telif action_space.__class__.__name__ == \"Box\":\n",
    "\t\t\tnum_outputs = action_space.shape[0]\n",
    "\t\t\tself.dist = DiagGaussian(self.base.output_size, num_outputs)\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif dataset in ['mnist', 'cifar10']:\n",
    "\t\t\tself.clf = Categorical(self.base.output_size, 2)#10)\n",
    "\n",
    "\t\tself.state_size = self.base.state_size\n",
    "\n",
    "\tdef forward(self, inputs, states, masks):\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef act(self, inputs, states, masks, deterministic=False):\n",
    "\t\tactor_features, states = self.base(inputs, states, masks)\n",
    "\t\tself.actor_features = actor_features\n",
    "\t\tdist = self.dist(actor_features)\n",
    "\t\tQ_values = dist.logits\n",
    "\t\tif deterministic:\n",
    "\t\t\taction = dist.mode()\n",
    "\t\telse:\n",
    "\t\t\taction = dist.sample()\n",
    "\n",
    "\t\taction_log_probs = dist.log_probs(action)\n",
    "\t\tif self.dataset in img_env.IMG_ENVS:\n",
    "\t\t\tclf = self.clf(self.actor_features)\n",
    "\t\t\tclf_proba = clf.logits\n",
    "\t\t\tif deterministic:\n",
    "\t\t\t\tclassif = clf.mode()\n",
    "\t\t\telse:\n",
    "\t\t\t\tclassif = clf.sample()\n",
    "\t\t\taction = torch.cat([action, classif], 1)\n",
    "\t\t\taction_log_probs += clf.log_probs(classif)\n",
    "\n",
    "\t\treturn action, Q_values, clf_proba, action_log_probs, states #dist.logits = Q values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "The code of ReplayMemory is essentially from the [DQN tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "\tdef __init__(self, capacity):\n",
    "\t\tself.capacity = capacity\n",
    "\t\tself.memory = []\n",
    "\t\tself.position = 0\n",
    "\n",
    "\tdef push(self, *args):\n",
    "\t\t\"\"\"Saves a transition.\"\"\"\n",
    "\t\tif len(self.memory) < self.capacity:\n",
    "\t\t\tself.memory.append(None)\n",
    "\t\tself.memory[self.position] = Transition(*args)\n",
    "\t\tself.position = (self.position + 1) % self.capacity\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\treturn random.sample(self.memory, batch_size)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.memory)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "\t\t\t\t\t\t('state', 'action', 'next_state', 'reward', 'curr_label'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "-------------\n",
    "The code of optimization is inspired by the [DQN tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html).             \n",
    "\n",
    "We combine the 2 losses (classification and navigation) to the total loss and optimize the total loss.\n",
    "\n",
    "Note: For the 1-step setting, we train the classification head b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_myNet(net, curr_label, optimizer, BATCH_SIZE=128, optimize_clf=False):\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\ttransitions = memory.sample(BATCH_SIZE)\n",
    "\tbatch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "\tnon_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "\t\t\t\t\t\t\t\t\t\t  batch.next_state)), dtype=torch.uint8).to(device)\n",
    "\n",
    "\tnon_final_next_states = torch.stack([s for s in batch.next_state \\\n",
    "\t\tif s is not None]).to(device)\n",
    "\t# print ('non_final_next_states', non_final_next_states.shape)\n",
    "\tstate_batch = torch.stack(batch.state).to(device)\n",
    "\t# print ('state_batch.size', state_batch.size)\n",
    "\taction_batch = torch.stack(batch.action).to(device)\n",
    "\treward_batch = torch.cat(batch.reward).to(device)\n",
    "\n",
    "\n",
    "\t_, Q_values_batch, clf_proba_batch, _, _ = net.act(\n",
    "\t\tinputs=state_batch.float(),\n",
    "\t\tstates=state_batch, masks=state_batch[1])\n",
    "\n",
    "\tstate_action_values = Q_values_batch.gather(1, action_batch[:, 0].view(BATCH_SIZE,1))\n",
    "\tnext_state_values = torch.zeros(BATCH_SIZE).to(device)\n",
    "\n",
    "\t_, next_Q_values_batch, _, _, _= target_net.act(inputs=non_final_next_states.float(),states=non_final_next_states, masks=non_final_next_states[1])\n",
    "\n",
    "\tnext_state_values[non_final_mask] = next_Q_values_batch.max(1)[0].detach()\n",
    "\n",
    "\texpected_state_action_values = (next_state_values * GAMMA) + reward_batch # Compute the expected Q values\n",
    "\tloss_dist = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\tcurr_label_batch = torch.cat(batch.curr_label).to(device)\n",
    "\tloss_clf = F.nll_loss(clf_proba_batch, curr_label_batch)\n",
    "\n",
    "\ttotal_loss = loss_dist + loss_clf\n",
    "# \toptimizer_dist = optim.RMSprop(net.parameters())\n",
    "# \toptimizer_dist.zero_grad()\n",
    "# \ttotal_loss.backward()\n",
    "# \tfor param in net.dist.parameters():\n",
    "# \t\tparam.grad.data.clamp_(-1, 1)\n",
    "# \toptimizer_dist.step()\n",
    "\toptimizer.zero_grad()\n",
    "\ttotal_loss.backward()\n",
    "\tfor param in net.parameters():\n",
    "\t\tparam.grad.data.clamp_(-1, 1)\n",
    "\toptimizer.step()\n",
    "\n",
    "\treturn total_loss, loss_clf, loss_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ....\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-4a3f3519f3a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mobs_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \t\taction_space=env.action_space, dataset='mnist').to(device)\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtarget_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_net' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tBATCH_SIZE = 128\n",
    "\tNUM_STEPS = 10\n",
    "\tGAMMA = 1 - (1 / NUM_STEPS) # Set to horizon of max episode length\n",
    "\tEPS = 0.05\n",
    "\tNUM_LABELS = 2\n",
    "\tWINDOW_SIZE = 14\n",
    "\tNUM_EPISODES = 1000\n",
    "\tTARGET_UPDATE = 10\n",
    "\n",
    "\tenv = img_env.ImgEnv('mnist', train=True, max_steps=NUM_STEPS, channels=2, window=WINDOW_SIZE, num_labels=NUM_LABELS)\n",
    "\t\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\tnet = myNet(\\\n",
    "\t\tobs_shape=env.observation_space.shape, \\\n",
    "\t\taction_space=env.action_space, dataset='mnist').to(device)\n",
    "\ttarget_net = myNet(\\\n",
    "\t\tobs_shape=env.observation_space.shape, \\\n",
    "\t\taction_space=env.action_space, dataset='mnist').to(device)\n",
    "\ttarget_net.load_state_dict(net.state_dict())\n",
    "\ttarget_net.eval()\n",
    "\tmemory = ReplayMemory(10000)\n",
    "    \n",
    "\ttotal_rewards = {}\n",
    "\tepisode_durations = {}\n",
    "\tloss_classification = {}\n",
    "\toptimizer_clf = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "    \n",
    "\n",
    "\tfor i_episode in range(NUM_EPISODES):\n",
    "\t\ttotal_reward_i = 0\n",
    "\t\tobservation = env.reset()\n",
    "\t\tcurr_label = env.curr_label.item()\n",
    "\t\tfor t in range(NUM_STEPS): # allow 100 steps\n",
    "\t\t\tactionS, Q_values, clf_proba, action_log_probs, states = net.act(inputs=torch.from_numpy(observation).float().resize_(1, 2, 32, 32).to(device), \\\n",
    "\t\t\t\tstates=observation, masks=observation[1])\n",
    "\t\t\tactionS = actionS.cpu().numpy()[0]\n",
    "\t\t\tclass_pred = actionS[1]\n",
    "\t\t\tlast_observation = observation\n",
    "\t\t\trand = np.random.rand()\n",
    "\t\t\tif rand < EPS:\n",
    "\t\t\t\tactionS = np.array(\n",
    "\t\t\t\t\t[np.random.choice(range(4)), np.random.choice(range(NUM_LABELS))])\n",
    "\t\t\taction = actionS[0]\n",
    "\t\t\tobservation, reward, done, info = env.step(actionS)\n",
    "\t\t\ttotal_reward_i = reward + GAMMA*total_reward_i\n",
    "\t\t\tmemory.push(torch.from_numpy(last_observation), torch.from_numpy(actionS), \\\n",
    "\t\t\t\ttorch.from_numpy(observation), torch.tensor([reward]).float(), torch.tensor([curr_label]))\n",
    "# \t\t\tprint ('t = %i: action = %i, class = %i, class_pred = %i, reward = %f'%(t, action, curr_label, class_pred, reward))\n",
    "\t\t\toptimize_myNet(net, curr_label, optimizer_clf, BATCH_SIZE)\n",
    "\n",
    "\t\t\tif done:\n",
    "# \t\t\t\t# print ('Done after %i steps'%(t+1))\n",
    "\t\t\t\tbreak\n",
    "    \n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "\t\tif i_episode % TARGET_UPDATE == 0:\n",
    "\t\t\ttarget_net.load_state_dict(net.state_dict())\n",
    "            \n",
    "\t\tloss_classification_i = F.nll_loss(clf_proba, env.curr_label.unsqueeze_(dim=0).to(device))\n",
    "\t\tif curr_label in total_rewards.keys():\n",
    "\t\t\ttotal_rewards[curr_label].append(total_reward_i)\n",
    "\t\t\tepisode_durations[curr_label].append(t)\n",
    "\t\t\tloss_classification[curr_label].append(loss_classification_i)\n",
    "\t\telse:\n",
    "\t\t\ttotal_rewards[curr_label] = [total_reward_i]\n",
    "\t\t\tepisode_durations[curr_label] = [t]\n",
    "\t\t\tloss_classification[curr_label] = [loss_classification_i]\n",
    "\tplt.title('Class 0')\n",
    "\tplt.subplot(3, 1, 1)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Episode_Duration')\n",
    "\tdurations_t = torch.tensor(episode_durations[0], dtype=torch.float)\n",
    "\tplt.plot(smoothing_average(durations_t.numpy()))\n",
    "\n",
    "\tplt.subplot(3, 1, 2)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Rewards')\n",
    "\ttotal_rewards_t = torch.tensor(total_rewards[0], dtype=torch.float)\n",
    "\tplt.plot(smoothing_average(total_rewards_t.numpy()))\n",
    "    \n",
    "\tplt.subplot(3, 1, 3)\n",
    "\tplt.ylim(top=1)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Loss Classification')\n",
    "\tloss_classification_t = torch.tensor(loss_classification[0], dtype=torch.float)\n",
    "\tplt.plot(smoothing_average(loss_classification_t.numpy()))\n",
    "\tplt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
