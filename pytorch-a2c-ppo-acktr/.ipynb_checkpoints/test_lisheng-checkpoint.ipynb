{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduce Natural Image Experiments using DQN\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from distributions import Categorical, DiagGaussian\n",
    "from collections import namedtuple\n",
    "\n",
    "import img_env \n",
    "\n",
    "import utils\n",
    "\n",
    "import model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from random import randint\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model\n",
    "-------------\n",
    "I keep almost the same architecture as the original code (model.py), except:\n",
    "* I expose Q values (=dist.logits) and classification probability (=clf.logits) to ease the computation of Q values and losses in optimization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(nn.Module):\n",
    "\tdef __init__(self, obs_shape, action_space, recurrent_policy=False, dataset=None, resnet=False, pretrained=False):\n",
    "\t\tsuper(myNet, self).__init__()\n",
    "\t\tself.dataset = dataset\n",
    "\t\tif len(obs_shape) == 3: #our mnist case\n",
    "\t\t\tself.base = model.CNNBase(obs_shape[0], recurrent_policy, dataset=dataset)\n",
    "\t\telif len(obs_shape) == 1:\n",
    "\t\t\tassert not recurrent_policy, \\\n",
    "\t\t\t\t\"Recurrent policy is not implemented for the MLP controller\"\n",
    "\t\t\tself.base = MLPBase(obs_shape[0])\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif action_space.__class__.__name__ == \"Discrete\": # our case\n",
    "\t\t\tnum_outputs = action_space.n\n",
    "\t\t\tself.dist = Categorical(self.base.output_size, num_outputs)\n",
    "\t\telif action_space.__class__.__name__ == \"Box\":\n",
    "\t\t\tnum_outputs = action_space.shape[0]\n",
    "\t\t\tself.dist = DiagGaussian(self.base.output_size, num_outputs)\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError\n",
    "\n",
    "\t\tif dataset in ['mnist', 'cifar10']:\n",
    "\t\t\tself.clf = Categorical(self.base.output_size, 10)\n",
    "\n",
    "\t\tself.state_size = self.base.state_size\n",
    "\n",
    "\tdef forward(self, inputs, states, masks):\n",
    "\t\traise NotImplementedError\n",
    "\n",
    "\tdef act(self, inputs, states, masks, deterministic=False):\n",
    "\t\tvalue, actor_features, states = self.base(inputs, states, masks)\n",
    "\t\tself.actor_features = actor_features\n",
    "\t\tdist = self.dist(actor_features) \n",
    "\t\tQ_values = dist.logits\n",
    "\t\tif deterministic:\n",
    "\t\t\taction = dist.mode()\n",
    "\t\telse:\n",
    "\t\t\taction = dist.sample()\n",
    "\n",
    "\t\taction_log_probs = dist.log_probs(action)\n",
    "\t\tif self.dataset in img_env.IMG_ENVS:\n",
    "\t\t\tclf = self.clf(self.actor_features)\n",
    "\t\t\tclf_proba = clf.logits\n",
    "\t\t\tif deterministic:\n",
    "\t\t\t\tclassif = clf.mode()\n",
    "\t\t\telse:\n",
    "\t\t\t\tclassif = clf.sample()\n",
    "\t\t\taction = torch.cat([action, classif], 1)\n",
    "\t\t\taction_log_probs += clf.log_probs(classif)\n",
    "\n",
    "\t\treturn value, action, Q_values, clf_proba, action_log_probs, states #dist.logits = Q values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory\n",
    "-------------\n",
    "The code of ReplayMemory is essentially from the [DQN tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'curr_label')) \n",
    "# curr_label is added to help computing the classification loss in optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization\n",
    "-------------\n",
    "The code of optimization is inspired by the [DQN tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html).             \n",
    "\n",
    "Main changes include:\n",
    "* \"Double optimization\": Since myNet has 2 heads sharing a same CNN part (one for action, the other for classification), it is possible to train both. Here I use RMSprop to optimize the action, and SGD to optimize the classification. The opt. of clf can be turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_myNet(net, curr_label, BATCH_SIZE=64, optimize_clf=False):\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\ttransitions = memory.sample(BATCH_SIZE)\n",
    "\tbatch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "\tnon_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.uint8)\n",
    "\n",
    "\tnon_final_next_states = torch.stack([s for s in batch.next_state \\\n",
    "    \tif s is not None])\n",
    "\t# print ('non_final_next_states', non_final_next_states.shape)\n",
    "\tstate_batch = torch.stack(batch.state)\n",
    "\t# print ('state_batch.size', state_batch.size)\n",
    "\taction_batch = torch.stack(batch.action)\n",
    "\treward_batch = torch.cat(batch.reward)\n",
    "\t\n",
    "\n",
    "\t_, _, Q_values_batch, clf_proba_batch, _, _ = net.act(inputs=state_batch.float(), \\\n",
    "\t\t\tstates=state_batch, masks=state_batch[1])\n",
    "\n",
    "\t# print (action_batch.shape)\n",
    "\tstate_action_values = Q_values_batch.gather(1, action_batch[:, 0].view(BATCH_SIZE,1))\n",
    "\t# actual Q values = Q values indexed by sampled action\n",
    "\tnext_state_values = torch.zeros(BATCH_SIZE)\n",
    "\t\n",
    "\t_, _, next_Q_values_batch, _, _, _= net.act(inputs=non_final_next_states.float(),states=non_final_next_states, masks=non_final_next_states[1])\n",
    "\t\n",
    "\tnext_state_values[non_final_mask] = next_Q_values_batch.max(1)[0].detach()\n",
    "\n",
    "\texpected_state_action_values = (next_state_values * GAMMA) + reward_batch # Compute the expected Q values\n",
    "\tloss_dist = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\toptimizer_dist = optim.RMSprop(net.parameters())\n",
    "\toptimizer_dist.zero_grad()\n",
    "\tif optimize_clf:\n",
    "\t\tloss_dist.backward(retain_graph=True)\n",
    "\telse:\n",
    "\t\tloss_dist.backward()\n",
    "\tfor param in net.dist.parameters():\n",
    "\t\tparam.grad.data.clamp_(-1, 1)\n",
    "\t\t# print (param.grad.data)\n",
    "\toptimizer_dist.step()\n",
    "\n",
    "\tif optimize_clf:\n",
    "\t\tcurr_label_batch = torch.cat(batch.curr_label)\n",
    "\t\tloss_clf = F.nll_loss(clf_proba_batch, curr_label_batch)\n",
    "\t\toptimizer_clf = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "\t\toptimizer_clf.zero_grad()\n",
    "\t\tloss_clf.backward()\n",
    "\t\tfor param in net.dist.parameters():\n",
    "\t\t\tparam.grad.data.clamp_(-1, 1)\n",
    "\t\toptimizer_clf.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot helper function\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ....\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "num_inputs 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9aeae26c0aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# train clf head every 5 time steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                 \u001b[0;31m# print (\"=============also train clf====\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                 \u001b[0mloss_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_myNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_clf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                 \u001b[0mloss_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_myNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_clf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\t\n",
    "\tenv = img_env.ImgEnv('mnist', train=True, max_steps=200, channels=2, window=5)\n",
    "\tnum_episodes = 200\n",
    "\n",
    "\n",
    "\tnet = myNet(obs_shape=env.observation_space.shape, action_space=env.action_space, dataset='mnist')\n",
    "\tmemory = ReplayMemory(10000)\n",
    "\n",
    "\tBATCH_SIZE = 32\n",
    "\tGAMMA = 0.999\n",
    "\n",
    "\ttotal_rewards = {}\n",
    "\tepisode_durations = {}\n",
    "\tfor i_episode in range(num_episodes):\n",
    "\t\ttotal_reward_i = 0\n",
    "\t\tobservation = env.reset()\n",
    "\t\tcurr_label = env.curr_label.item()\n",
    "\t\tfor t in range(100): # allow 100 steps\n",
    "\t\t\tvalue, actionS, Q_values, clf_proba, action_log_probs, states = net.act(inputs=torch.from_numpy(observation).float().resize_(1, 2, 32, 32), \\\n",
    "\t\t\t\tstates=observation, masks=observation[1])\n",
    "\t\t\taction = actionS.numpy()[0][0]\n",
    "\t\t\tclass_pred = actionS.numpy()[0][1]\n",
    "\t\t\tactionS = actionS.numpy()[0]\n",
    "\t\t\tlast_observation = observation\n",
    "\t\t\tobservation, reward, done, info = env.step(actionS)\n",
    "\t\t\ttotal_reward_i = reward + GAMMA*total_reward_i\n",
    "\t\t\tmemory.push(torch.from_numpy(last_observation), torch.from_numpy(actionS), \\\n",
    "\t\t\t\ttorch.from_numpy(observation), torch.tensor([reward]).float(), torch.tensor([curr_label]))\n",
    "\t\t\t# print ('t = %i: action = %i, class = %i, class_pred = %i, reward = %f'%(t, action, curr_label, class_pred, reward))\n",
    "\n",
    "\t\t\t # train action head every time\n",
    "\n",
    "\t\t\tif t % 5 == 0: # train clf head every 5 time steps\n",
    "\t\t\t\t# print (\"=============also train clf====\")\n",
    "\t\t\t\toptimize_myNet(net, curr_label, BATCH_SIZE, optimize_clf=True)\n",
    "\t\t\telse:\n",
    "\t\t\t\toptimize_myNet(net, curr_label, BATCH_SIZE, optimize_clf=False)\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tprint ('Done after %i steps'%(t+1))\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\tif curr_label in total_rewards.keys():\n",
    "\t\t\ttotal_rewards[curr_label].append(total_reward_i)\n",
    "\t\t\tepisode_durations[curr_label].append(t)\n",
    "\t\telse:\n",
    "\t\t\ttotal_rewards[curr_label] = [total_reward_i]\n",
    "\t\t\tepisode_durations[curr_label] = [t]\n",
    "\n",
    "\tplt.title('Class 0')\n",
    "\tplt.subplot(2, 1, 1)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Episode_Duration')\n",
    "\tdurations_t = torch.tensor(episode_durations[0], dtype=torch.float)\n",
    "\tplt.plot(durations_t.numpy())\n",
    "\n",
    "\tplt.subplot(2, 1, 2)\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.ylabel('Rewards')\n",
    "\ttotal_rewards_t = torch.tensor(total_rewards[0], dtype=torch.float)\n",
    "\tplt.plot(total_rewards_t.numpy())\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
